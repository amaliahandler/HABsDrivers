---
title: "HAB Drivers Data Compilation"
author: "Amalia Handler"
date: "1/5/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

## Script Purpose and Background

Compiling data from the 2007, 2012, and 2017 NLA and data associated with the watersheds for each of these lakes from LakeCat and National Nutrient Inventory, in addition to NLA derived data.

```{r, include = FALSE, warning = FALSE, message = FALSE}
# Load packages for the script
library(devtools)
library(dplyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(tidyr)
library(stars)
```

#### NLA Data

Physical, chemical, and algal (phytoplankton) data from the NLA. These data were compiled by Karen Blocksom.


```{r}

# Updated from Karen to include temperature data whenever a profile was collected
all_nla <- read.csv("./inst/source_data/NLA2007-2012-2017_LakeData_forHABs_28Sept2023.csv")

# Note that one site (UNIQUE_ID = "NLA_NV-10003") is creating problems since it was sampled twice in 2007 and both visits are coded a VISIT_NO = 1. Quick fix is to recode the site visit number in September to be VISIT_NO = 2.
all_nla$VISIT_NO[all_nla$UNIQUE_ID == "NLA_NV-10003" & all_nla$DATE_COL == "09/17/2007"] <- 2

# Read in NLA UNIQUE_ID - NHDPlusV2 COMID crosswalk info from Marc
nla_lakecat <- read.csv("./inst/source_data/Integrated_NLA-LakeCat_COMID_Crosswalk.csv")

# Crosswalk document that contains the NLA SITE_ID and the NHDPlusV2 COMID
int_nla <- read.csv("./inst/source_data/NLA_Integrated_Design_Status_20190821.csv")

# Find the distinct UNIQUE_ID and COMIDs for the NLA-Lakecat crosswalk
unique_nla_lakecat <- nla_lakecat %>%
  select(UNIQUE_ID, COMID) %>%
  distinct()

# Define needed columns from NLA data (will deal with missing pH values below)
nla_comid <- all_nla %>%
  left_join(unique_nla_lakecat, by = "UNIQUE_ID")

# For sites still missing COMIDs, get from the alternate integrated NLA document
unique_nla_int <- int_nla %>%
  select(UNIQUE_ID, COMID) %>%
  distinct()

missing_coms <- nla_comid %>%
  filter(is.na(COMID)) %>%
  select(!COMID) %>%
  left_join(unique_nla_int, by = "UNIQUE_ID")

nla_all_coms <- nla_comid %>%
  filter(is.finite(COMID)) %>%
  bind_rows(missing_coms)

# Pivot from wide to long format
# Problem with some ammonium numbers recorded as "< 0.03", which cannot be coerced to a numeric value. Need to replace these values with something so that the column can be coerced to numeric type.
# Stratified column is a character currently and needs to be changed to numeric type to convert to long format
# Lots of nitrate data below the MDL. Looks like everything below the reporting limit had a result recorded as 0.005 mg/L in 2007 (RL = 0.02). In 2012 (RL = 0.02) and 2017 (RL = 0.0025) ND for nitrate is reported as NA in result column. For NITRATE, values of 0.005 in 2007 are what was reported for anything below the reporting limit of 0.02. Assuming this is 1/2 of the MDL in 2007. 
# When NITRATE and AMMONIUM are below the detection limit, replace with 1/2 the MDL.
# 
nla_long <- nla_all_coms %>%
  mutate(
    STRATIFIED = ifelse(is.na(STRATIFIED), 0, 1),
    AMMONIA_N = as.numeric(replace(AMMONIA_N, AMMONIA_N == "< 0.03", 0.5 * 0.03)),
    AMMONIA_N = ifelse(is.na(AMMONIA_N) & grepl("ND", AMMONIA_N_NARS_FLAG), 0.5 * AMMONIA_N_MDL, AMMONIA_N),
    NITRATE_N = ifelse(is.na(NITRATE_N) & grepl("ND", NITRATE_N_NARS_FLAG), 0.5 * NITRATE_N_MDL, NITRATE_N),
    MICX_DET = ifelse(is.na(MICX) & grepl("ND", MICX_NARS_FLAG), 0, MICX),
  ) |>
  select(SITE_ID, VISIT_NO, UNIQUE_ID, DSGN_CYCLE, DATE_COL, LAT_DD83, LON_DD83, COMID,
         TEMPERATURE, MAXDEPTH, STRATIFIED, AMMONIA_N, DO_SURF, DOC, NTL, PTL,
         TURB, NITRATE_N, PH, CHLA_RESULT, MICX, B_G_DENS) %>%
  pivot_longer(TEMPERATURE:B_G_DENS, names_to = "ANALYTE", values_to = "RESULT")

# Where PH data was not measured in the lab, retrieve the top-most pH measurement from the profile
ph_profile <- nla_long %>%
  filter(ANALYTE == "PH") %>%
  filter(is.na(RESULT)) %>%
  select(-RESULT) %>%
  left_join(select(all_nla, VISIT_NO:DSGN_CYCLE, PH_FIELD), 
            by = c("UNIQUE_ID", "DSGN_CYCLE", "VISIT_NO")) %>%
  rename(RESULT = PH_FIELD)

# Remove the rows with missing pH data from the lab and add the rows with pH data from the profile data
# Add a units columns
nla_long <- nla_long %>%
  filter(!(ANALYTE == "PH" & is.na(RESULT))) %>%
  bind_rows(ph_profile) %>%
  mutate(UNITS = case_when(ANALYTE == "TEMPERATURE" ~ "DEG_C",
                           ANALYTE == "MAXDEPTH" ~ "M",
                           ANALYTE == "STRATIFIED" ~ "Y/N_1/0",
                           ANALYTE %in% c("PTL", "MICX") ~ "UG/L",
                           ANALYTE == "TURB" ~ "NTU",
                           ANALYTE == "B_G_DENS" ~ "CELLS/ML",
                           ANALYTE == "PH" ~ "STD_UNITS",
                           ANALYTE %in% c("DO_SURF", "AMMONIA_N", "DOC", "NTL", "NITRATE_N", "CHLA_RESULT") ~ "MG/L"))

# Extract NLA observation ID information for joining with other data sources
nla_id <- nla_long %>%
  select(SITE_ID:COMID) %>%
  distinct()

```


#### NLA Evaporation/Inflow

Add evaporation to inflow ratio data from Emi Fergus and Renee Brooks.

```{r}

# Load data
ratio0712 <- read.csv("./inst/source_data/NLA2007-2012_E-I_HYDRAP.csv")
ratio17   <- read.csv("./inst/source_data/NLA17_EI_estimates-Final.csv")

# Transform data to long form
ei0712 <- ratio0712 %>%
  select(SITE_ID, VISIT_NO, YEAR, E_I, d_excess) %>%
  rename(DSGN_CYCLE = YEAR,
         EVAP_INFL = E_I,
         D_EXCESS = d_excess) %>%
  pivot_longer(EVAP_INFL:D_EXCESS, names_to = "ANALYTE", values_to = "RESULT") |>
  left_join(nla_id, by = c("SITE_ID", "VISIT_NO", "DSGN_CYCLE")) %>%
  mutate(UNITS = case_when(ANALYTE == "EVAP_INFL" ~ "RATIO",
                           ANALYTE == "D_EXCESS" ~ "PERMIL"))

ei17 <- ratio17 %>%
  select(SITE_ID, E_I, d_excess, VISIT_NO) %>%
  mutate(DSGN_CYCLE = 2017) %>%
    rename(EVAP_INFL = E_I,
         D_EXCESS = d_excess) %>%
  pivot_longer(EVAP_INFL:D_EXCESS, names_to = "ANALYTE", values_to = "RESULT") |>
  left_join(nla_id, by = c("SITE_ID", "VISIT_NO", "DSGN_CYCLE")) %>%
  mutate(UNITS = case_when(ANALYTE == "EVAP_INFL" ~ "RATIO",
                           ANALYTE == "D_EXCESS" ~ "PERMIL"))
  
# Bind together 2007/2012 with 2017 data
evap_infl <- rbind(ei0712, ei17)

# Rearrange columns
evap_infl <- evap_infl |>
  relocate(SITE_ID, VISIT_NO, UNIQUE_ID, DSGN_CYCLE, DATE_COL, LAT_DD83, LON_DD83, COMID, ANALYTE, RESULT, UNITS) |>
  # There are 3 rows with no UNIQUE_ID that have no data. They are VISIT_NO = 2 for one site that was only visited once (SITE_ID = NLA12_MI-107) and two sites (NLA12_OAHU12-12, NLA12_OAHU12-12) that have no record in the NLA data I received.
  filter(!is.na(UNIQUE_ID))

# From Emi and Renee: NA values mean that E/I was not calculated for some reason. Generally missing data or unrealistic E/I values were removed
# # Investigating missing values
# filter(ratio0712, is.na(E_I))
# filter(ratio17, is.na(E_I))
#
# write.csv(filter(ratio0712, is.na(E_I)), "missing2012-E_I.csv", row.names = F)
# write.csv(filter(ratio17, is.na(E_I)), "missing2017-E_I.csv", row.names = F)
# 
# filter(ei0712, is.na(COMID))
# filter(ei17, is.na(COMID))

# How many of the NLA sites are missing E/I data?
nrow(nla_id) - nrow(evap_infl)
nrow(nla_id) - nrow(ei17) - nrow(ei0712) # Didn't lose anything in the joining process

# There were fewer rows for the E/I data than there exist for the full NLA data. Unclear why, the E/I data includes site revisits and hand selected sites. Cannot identify a systematic reason for their exclusion.

```


#### NLCD

Add NLCD compiled by Marc Weber for all NLA 2007 and 2012 lakes (in Lakecat and not in Lakecat). Note that Marc did not include data from 2016 NLCD to pair with 2017 NLA. The code below also brings in the 2016 NLCD from LakeCat for 2017 NLA lakes included in LakeCat.

```{r}
# Watershed metrics for NLA lakes compiled by Marc
wsmet <- read.csv("./inst/source_data/NLA_WatershedMetrics.csv")

# There is a duplicated set of data for one lake (UNIQUE_ID = "NLA_NV_10003") that has two different SITE_IDs for 2007. LakeCat data is identical between the two. Easiest solution is to remove one row of the data. Removing the one with the hand selected SITE_ID
wsmet <- filter(wsmet, SITE_ID != "NLA06608-NV:4")

# Focus on NLCD first for 2007 and 2012 NLA (pairing with 2006 and 2011 NLCD)
nla0712_nlcd <- wsmet %>%
  mutate(dev_ws = case_when(DSGN_CYCLE == 2007 ~ rowSums(select(wsmet, PctUrbOp2006Ws:PctUrbHi2006Ws)),
                            TRUE ~ rowSums(select(wsmet, PctUrbOp2011Ws:PctUrbHi2011Ws))),
         wet_ws = case_when(DSGN_CYCLE == 2007 ~ PctWdWet2006Ws + PctHbWet2006Ws,
                            TRUE ~ PctWdWet2011Ws + PctHbWet2011Ws),
         fst_ws = case_when(DSGN_CYCLE == 2007 ~ rowSums(select(wsmet, c(PctDecid2006Ws:PctMxFst2006Ws))),
                               TRUE ~ rowSums(select(wsmet, c(PctDecid2011Ws:PctMxFst2011Ws)))),
         agr_ws = case_when(DSGN_CYCLE == 2007 ~ PctHay2006Ws + PctCrop2006Ws,
                            TRUE ~ PctHay2011Ws + PctCrop2011Ws)) |>
  filter(wsmet$DSGN_CYCLE != 2017) |>
  select(UNIQUE_ID, DSGN_CYCLE, ends_with("_ws")) %>%
  right_join(filter(nla_id, DSGN_CYCLE != 2017), by = c("UNIQUE_ID", "DSGN_CYCLE"))

# Next focus on 2017 NLA to pair with 2016 NLCD data
# Load in NLCD 2016 data
nlcd16 <- readr::read_csv("./inst/source_data/LakeCat_NLCD2016.csv")

# Find COMIDS for 2017 NLA
nla_id17 <- nla_id |>
  filter(DSGN_CYCLE == 2017) |>
  pull(COMID) |>
  unique()

# Filter to NLA lake IDs in LakeCat for 2017
nla17_nlcd_lakecat <- nla17_lakecat_ids <- nla_id |>
  filter(DSGN_CYCLE == 2017) |>
  filter(COMID %in% nlcd16$COMID)

# Calculate land use category sums
nlcd16_sums <- nlcd16 %>%
  mutate(agr_ws = PctHay2016Ws + PctCrop2016Ws,
         dev_ws = rowSums(select(nlcd16, PctUrbOp2016Ws:PctUrbHi2016Ws)),
         fst_ws = rowSums(select(nlcd16, c(PctDecid2016Ws:PctMxFst2016Ws))),
         wet_ws = PctWdWet2016Ws + PctHbWet2016Ws) %>%
  filter(COMID %in% nla17_nlcd_lakecat$COMID) %>%
  select(COMID, ends_with("_ws"))   

# Join with NLA ID info
nla17_nlcd16_lakecat <- right_join(nlcd16_sums, nla17_lakecat_ids, by = "COMID")

# Bind together 2007/2012 and 2017 NLA-NLCD data
# nla_nlcd_lakecat <- nla0712_nlcd |>
#   bind_rows(nla17_nlcd) |>
#   select(UNIQUE_ID, DSGN_CYCLE, VISIT_NO, ends_with("_ws"))

# Binding rows takes place in code chunk below where missing lakes from LakeCat are pulled in a processed with NLCD data directly

```


Pull in 2016 NLCD data for watersheds missing from LakeCat for 2017 NLA


```{r}
# 2016 NLCD data
rast <- read_stars('O:/PRIV/CPHEA/PESD/COR/CORFILES/Geospatial_Library_Projects/StreamCat/LandscapeRasters/QAComplete/nlcd_2006_land_cover_l48_20210604.tif')

# Watershed areas for 2017 NLA lakes
ws17 <- read_sf(dsn='O:/PRIV/CPHEA/PESD/COR/CORFiles/Geospatial_Library_Resource/PHYSICAL/WATERSHEDS/NLA2017_Basins/NLA2017_AllBasins.gdb', layer='NLA17_Basins')

# ID the lakes that were not covered by LakeCat
missing_site_ids <- nla_id |>
  filter(DSGN_CYCLE == 2017) |>
  filter(!(COMID %in% nlcd16$COMID)) |>
  pull(SITE_ID) |>
  unique()

# Still short 14 lakes?
missing_lakes <- filter(ws17, SITE_ID %in% missing_site_ids)

# Ensure they are in the same projection
missing_lakes <- st_transform(missing_lakes, st_crs(rast))

# Try renaming the column holding the geometry
missing_lakes <- rename(missing_lakes, geometry = Shape)

# Check graphically that there is data within the basin
# i <- 81
# lk_pix <- st_crop(rast, missing_lakes[i,], as_points = FALSE)
# lk_pix <- st_as_stars(lk_pix)
# plot(missing_lakes[i,]$geometry)
# plot(lk_pix, add = T)
# plot(missing_lakes[i,]$geometry, add = T)

# Issues with basin for COMID 133250246 SITE_ID NLA17_KS_10011. Some kind of weird multisurface geometry that refuses to be transformed into a multipolygon. Removing for now.
missing_lakes <- filter(missing_lakes, !SITE_ID == "NLA17_KS-10011")

# Extract NLCD for each missing watershed area
missing_nlcd <- do.call(rbind, lapply(c(1:nrow(missing_lakes)), function(i){
  print(i)
  
  # Polygon of the missing lake's watershed area
  lake <- missing_lakes[i,]
  
  # Crop the NLCD 2016 data, as_points = F to include all pixels intersecting the basin shape
  ws_pix <- st_crop(rast, missing_lakes[i,], as_points = FALSE)
  
  # Convert from stars proxy to stars object and create table of NLCD pixel counts within watershed boundary
  nlcd_extract <- table(st_as_stars(ws_pix))

  # Preemptively rename the attributes to match that in LakeCat
  names(nlcd_extract) <- c("Unclassified", "PctOw2016Ws", "PctIce2016Ws", "PctUrbOp2016Ws", "PctUrbLo2016Ws", "PctUrbMd2016Ws", "PctUrbHi2016Ws", "PctBl2016Ws", "PctDecid2016Ws", "PctConif2016Ws", "PctMxFst2016Ws", "PctShrb2016Ws", "PctGrs2016Ws", "PctHay2016Ws", "PctCrop2016Ws", "PctWdWet2016Ws", "PctHbWet2016Ws")
  
  # Calculate total area included in the basin
  total_area = sum(nlcd_extract) * 900
  
  # Calculate proportions of each land cover type and convert to wide format to match LakeCat
  nlcd_wide <- nlcd_extract |>
    as.data.frame() |>
    rename(cover_type = 1) |>
    filter(!cover_type == "Unclassified") |>
    mutate(props = (Freq * 900) / total_area * 100, .keep = "unused") |>
    tidyr::pivot_wider(names_from = cover_type, values_from = props) |>
    mutate(SITE_ID = missing_lakes$SITE_ID[i],
           COMID = missing_lakes$COMID[i], .before = 1) 
  
  return(nlcd_wide)
}))

# Sum into categories and join with NLA ID
# Note: one of the sites was visited twice in 2017, so this produces a dataframe of nrow(missing_nlcd) + 1
# Note: Two sites ("NLA17_MO-HP005", "NLA17_TX-HP005") are hand picked sites and lack COMIDs (NLA COMID = 0)
missing_nlcd_sums <- missing_nlcd %>%
  mutate(agr_ws = PctHay2016Ws + PctCrop2016Ws,
         dev_ws = rowSums(select(missing_nlcd, PctUrbOp2016Ws:PctUrbHi2016Ws)),
         fst_ws = rowSums(select(missing_nlcd, c(PctDecid2016Ws:PctMxFst2016Ws))),
         wet_ws = PctWdWet2016Ws + PctHbWet2016Ws) |>
  select(SITE_ID, ends_with("_ws")) |>
  left_join(filter(nla_id, DSGN_CYCLE == 2017), by = c("SITE_ID"))

# There's one more row than expected due to two site visits for one of the lakes
dup_site <- missing_nlcd_sums$SITE_ID[duplicated(missing_nlcd_sums$SITE_ID)]
filter(missing_nlcd_sums, SITE_ID == dup_site)

# Two sites ("NLA17_MO-HP005", "NLA17_TX-HP005") are hand picked sites and lack COMIDs (NLA COMID = 0)
filter(missing_nlcd_sums, SITE_ID %in% c("NLA17_MO-HP005", "NLA17_TX-HP005"))
filter(missing_lakes, SITE_ID %in% c("NLA17_MO-HP005", "NLA17_TX-HP005"))
filter(nla_id, SITE_ID %in% c("NLA17_MO-HP005", "NLA17_TX-HP005"))

# Bind together with other NLCD data
nla_nlcd <- missing_nlcd_sums |>
  bind_rows(nla0712_nlcd, nla17_nlcd16_lakecat) |>
  select(UNIQUE_ID, DSGN_CYCLE, VISIT_NO, ends_with("_ws"))

```


#### PRISM Precipitation and Temperature

Match PRISM monthly precipitation and temperature data to NLA sample dates
Description:
* Precipitation (mm), cumulative precipitation for the specified month and year in which the sample was collected
* Temperature (degrees C), mean temperature for the month and year in which the sample was collected 


```{r}
# Get the year and the month that the sample was collected
yr_mon <- nla_id |>
  mutate(Month = format(as.Date(DATE_COL, format = "%m/%d/%Y"), "%m"))

# Convert watershed precip and temp data to long format for easier joining
monthly_clim <- wsmet |>
  select(UNIQUE_ID, DSGN_CYCLE, PRECIP_200701_PT:TMEAN_201709_PT) |>
  pivot_longer(cols = PRECIP_200701_PT:TMEAN_201709_PT,
               names_to = c("Type", "Year_Month", "PT"),
               names_sep = "_",
               values_to = "Value") |>
  mutate(Month = substr(Year_Month, 5,6),
         Year = as.integer(substr(Year_Month, 1,4))) |>
  filter(DSGN_CYCLE == Year) |>
  select(-c(PT, Year_Month, Year))

# Join the two
precip_temp <- yr_mon |>
  left_join(filter(monthly_clim, Type == "PRECIP"), by = c("UNIQUE_ID", "DSGN_CYCLE", "Month")) |>
  select(-Type) |>
  rename(precip_mean_month = Value) |>
  left_join(filter(monthly_clim, Type == "TMEAN"), by = c("UNIQUE_ID", "DSGN_CYCLE", "Month")) |>
  select(-Type) |>
  rename(temp_mean_month = Value)

```


#### LakeCat Watershed Characteristics

Pull in other watershed data compiled by Marc

```{r}
# Pull in other relevant watershed characteristics
ws_char <- wsmet |>
  select(UNIQUE_ID, DSGN_CYCLE, lakemorpho_depth, lakemorpho_fetch, lakemorpho_shoreline.length, BFIWs, AgKffactWs, KffactWs, RunoffWs, Precip_Minus_EVTWs, WWTPWs, Precip8110Ws, Tmean8110Ws, ElevWs, SlopeWs) %>%
  right_join(nla_id, by = c("UNIQUE_ID", "DSGN_CYCLE"))

```


#### National Nutrient Inventory

Add in nutrient inventory data compiled by Robert Sabo, Meredith Brehob, and others

```{r}
# Note that nutrient inventory data is in units of kg N/yr or kg P/yr. Need to divide by the watershed area in order to get the per-unit area inventory.

# Read in the 2007 data
nni2007 <- read.csv("./inst/source_data/NLA07_NutrientInventory.csv")

# Read in the 2012 data
nni2012 <- read.csv("./inst/source_data/NLA12_NutrientInventory.csv")

# Select the relevant columns from each and join with NLA ID info to get unique IDs
extractor <- function(x, year){
  x |> 
    select(SITE_ID, VISIT_NO, WSAREA, N_Total_Deposition_2002:Recovered_P_2012) |>
    left_join(filter(nla_id, DSGN_CYCLE == year), by = c("SITE_ID", "VISIT_NO"))
}

# Frustratingly, column names are not consistent across years from the NNI, have to manually fix a few of these
nni07 <- extractor(nni2007, 2007)

nni12 <- nni2012 |>
  extractor(2012) |>
  rename(N_Crop_N_Rem_2002 = N_Crop_N_Rem,
         N_Human_N_Demand_2012 = Human_N_Demand_2012)

nni0712 <- bind_rows(nni07, nni12)

# Note that 2 observations are missing a watershed area. Also, there are small difference in the area for the same watershed depending on if the measurement is coming from 2007 or 2012. 
duplicated_ids <- nni0712 |>
  as_tibble() |>
  filter(duplicated(UNIQUE_ID)) |>
  pull(UNIQUE_ID)

nni0712 |>
  as_tibble() |>
  filter(UNIQUE_ID %in% duplicated_ids)|>
  select(UNIQUE_ID, SITE_ID, VISIT_NO, DSGN_CYCLE, WSAREA) |>
  arrange(UNIQUE_ID)

# Extract the watershed area data, calculate means by UNIQUE_ID
areas <- nni0712 |>
  as_tibble() |>
  mutate(WSAREA_HA = round(WSAREA * 100, 2)) |>
  select(UNIQUE_ID, WSAREA_HA) |>
  group_by(UNIQUE_ID) |>
  summarise(WSAREA_HA = mean(na.omit(WSAREA_HA))) |>
  ungroup()

# Note that the same lake can have (usually small) differences in the nutrient inputs the same NNI year in the 2007 and 2012 NLA files. This is because the NNI was down-scaled to each lake watershed using the corresponding NLCD data for the NLA survey (2007 NLA = 2006 NLCD, 2012 NLA = 2011 NLCD). Because of slight changes in the relevant land use classes in the two NLCDs, there are differences in the derived inventories based on this.
nni_mean <- nni0712 |>
  # A bunch of variables were not named consistently among years or input types
  rename(N_Human_N_Demand_2007 = Human_N_Demand_2007,
         N_Human_N_Demand_2002 = Human_N_Demand_2002,
         P_Legacy_P_2002 = Legacy_P_2002,
         P_Recovered_P_2002 = Recovered_P_2002,
         P_Legacy_P_2007 = Legacy_P_2007,
         P_Recovered_P_2007 = Recovered_P_2007,
         P_Legacy_P_2012 = Legacy_P_2012,
         P_Recovered_P_2012 = Recovered_P_2012) |>
  pivot_longer(cols = N_Total_Deposition_2002:P_Recovered_P_2012,
               names_to = c("nutrient", "type", "year"),
               names_sep = c(1, -4),
               values_to = "amount") |>
  # Remove leading and trailing underscores in type column. 
  mutate(type = stringr::str_sub(type, 2, -2)) |>
  # Fix inconsistencies in how input types are labeled so these will group when calculating the 2002, 2007, 2012 mean
  mutate(type = case_when(type == "Livestock.Waste" ~ "livestock_Waste",
                          type == "Agriculture_NHy" ~ "Agricultural_NHy",
                          TRUE ~ type)) |>
  # What is Low P and High_P?
  # Missing N_Lightning_2002, N_AG_N2_2007, and P_Hi_P_2007. For now, removing these.
  filter(!type %in% c("Lightning", "AG_N2", "Hi_P")) |>
  # Removing a number of other variables on recommendation from Robert. These are generally variables used to derive other, but are sometimes variables that have a high degree on uncertainty.
  filter(!type %in% c("AG_N2O_INPUTS", "AG_N2O_SOIL", "Agricultural_NHy", "Livestock_Food_Demand", "Livestock_N_Content", "Combustion_transportation", "Combustion_utility", "Complete_Mass_Balance_AG", "Imp_Balance", "Imp_Ratio", "NUE_AG", "Other_emissions", "Low_P", "human_food_demand_kg", "human_nonfood_demand_kg", "kg_ww", "livestock_demand", "livestock_production")) |>
  # Established already that revisits have identical data
  filter(VISIT_NO == 1) |>
  # Calculate the mean of the three inventories
  group_by(UNIQUE_ID, nutrient, type) |>
  summarise(mean020712 = mean(amount, na.rm = T)) |>
  ungroup() |>
  # Normalize by the watershed area
  left_join(areas, by = "UNIQUE_ID") |>
  mutate(mean_kg_ha = mean020712 / WSAREA_HA) |>
  pivot_wider(id_cols = UNIQUE_ID,
              names_from = c(nutrient, type),
              values_from = mean_kg_ha)

```

#### Ecoregions

Identify the ecoregion for each NLA Lake

Data request to Marc: I’m looking for the ecoregions data to pair with the 2007, 2012, and 2017 NLAs. I’m thinking the level 3 ecoregions and the 3 and 9 aggregated ecoregions to start. I’m attaching the ID information for the set of lakes I have for this project.

```{r}
# Read in data
nla_ecor <- readr::read_csv("./inst/source_data/NLA_UniqueIDs_HABsDrivers_Ecoregions.csv") |>
  select(UNIQUE_ID:AG_ECO9_NM)

```


#### Final Data Compilation

Compile together NLA, NLA+, LakeCat, and Nutrient Inventory data

```{r}
# Start with NLA data
habs_comb <- nla_long %>%
  
  # Add evaporation and inflow data (NLA+)
  bind_rows(evap_infl) %>%
  select(!UNITS) %>%
  
  # Pivot from long to wide format
  pivot_wider(names_from = ANALYTE, values_from = RESULT, values_fill = NA) %>%
  
  # Add NLCD (from Marc, LakeCat, and self-compiled)
  left_join(nla_nlcd, by = c("UNIQUE_ID", "DSGN_CYCLE", "VISIT_NO")) %>%
  
  # Add the precipitation and temperature for the month in which the sample was collected
  left_join(select(precip_temp, UNIQUE_ID, DSGN_CYCLE, VISIT_NO, precip_mean_month, temp_mean_month), by = c("UNIQUE_ID", "DSGN_CYCLE", "VISIT_NO")) |>
  
  # Add watershed characteristics from LakeCat
  left_join(select(ws_char, UNIQUE_ID:SlopeWs, VISIT_NO), by = c("UNIQUE_ID", "DSGN_CYCLE", "VISIT_NO")) |>
  
  # Add nutrient inventory means
  left_join(nni_mean, by = "UNIQUE_ID") |>
  
  # Add the ecoregion of each lake
  left_join(select(nla_ecor, -LAT_DD83, -LON_DD83, -COMID), by = "UNIQUE_ID") |>
  
  # Format
  mutate(DSGN_CYCLE = factor(DSGN_CYCLE),
         UNIQUE_ID = factor(UNIQUE_ID),
         AG_ECO3 = factor(AG_ECO3),
         AG_ECO9_NM = factor(AG_ECO9_NM))

# Convert to a sf object
habs_nars_all <- sf::st_as_sf(habs_comb, coords = c('LON_DD83', 'LAT_DD83'), crs = 4269, remove = FALSE)

# Transform to an equal area projection so that x and y distances are equivalent
habs <- sf::st_transform(habs_nars_all, crs = 5070)

# Save the data as part of the package
usethis::use_data(habs, overwrite = T)

```

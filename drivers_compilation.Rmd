---
title: "HAB Drivers Compilation"
author: "Amalia Handler"
date: "1/5/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

Compiling NLA 2007, 2012, and 2017 cyanobacteria data

```{r, include = FALSE, warning = FALSE, message = FALSE}
library(devtools)
library(dplyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(tidyr)
library(stars)
```

List of variables to compile

National lakes assessment (NLA)
* X_Chlorophyll a
* X_Cyanobacteria abundance
* X_Microcystin
* X_Cylindrospermopsin
* X_Total nitrogen
* X_Total phosphorus
* X_Dissolved organic carbon
* X_Turbidity
* X_pH
* X_Water temperature
* Dissolved oxygen
* X_Stratification
* X_HydroAP
* X_Evaporation/Inflow

PRISM (climate normals)
* X_Air temperature
* X_Precipitation

LakeCat
* X_Urban land cover
* X_Agricultural land cover
* X_Forested/natural land cover
* X_Wetland land cover
* Burned area
* X_Slope
* X_Elevation
* X_Wastewater treatment plants
* X_Runoff
* Crop biological nitrogen fixation (CBNF)
* Synthetic fertilizer application rate (FERT)
* Mean rate of manure application (MANURE)
* Atmospheric ammonium deposition (NH4_2008)
* Atmospheric nitrate deposition (NO3_2008)

Nutrient Inventory
* Nitrogen and phosphorus inputs
* Nitrogen and phosphorus surplus

Lakemorpho
* X_Depth
* X_Fetch
* Surface area
* Perimeter

Load NLA data on cyanobacteria and water physiochemical characteristics
These data were compiled by Karen Blocksom


```{r}
# Read in data
# all_nla <- read.csv("NLA2007-2012-2017_LakeData_forHABs_4April2022.csv")

# Updated from Karen to include temperature data whenever a profile was collected
all_nla <- read.csv("NLA2007-2012-2017_LakeData_forHABs_12April2022.csv")

# Note that one site (UNIQUE_ID = "NLA_NV-10003") is creating problems since it was sampled twice in 2007 and both visits are coded a VISIT_NO = 1. Quick fix is to recode the site visit number in September to be VISIT_NO = 2.
all_nla$VISIT_NO[all_nla$UNIQUE_ID == "NLA_NV-10003" & all_nla$DATE_COL == "09/17/2007"] <- 2

# Read in NLA UNIQUE_ID - NHDPlusV2 COMID crosswalk info from Marc
nla_lakecat <- read.csv("Integrated_NLA-LakeCat_COMID_Crosswalk.csv")

# Crosswalk document that contains the NLA SITE_ID and the NHDPlusV2 COMID
int_nla <- read.csv("NLA_Integrated_Design_Status_20190821.csv")

# Find the distinct UNIQUE_ID and COMIDs for the NLA-Lakecat crosswalk
unique_nla_lakecat <- nla_lakecat %>%
  select(UNIQUE_ID, COMID) %>%
  distinct()

# Define needed columns
nla_comid <- all_nla %>%
  select(SITE_ID, VISIT_NO, UNIQUE_ID, DSGN_CYCLE, DATE_COL, LAT_DD83, LON_DD83, 
         TEMPERATURE, STRATIFIED, AMMONIA_N, DOC, NTL, PTL, TURB, 
         NITRATE_N, PH, CHLA_RESULT, MICX, CYLSPER, B_G_DENS) %>%
  left_join(unique_nla_lakecat, by = "UNIQUE_ID")

# For sites still missing COMIDs, get from the alternate integrated NLA document
unique_nla_int <- int_nla %>%
  select(UNIQUE_ID, COMID) %>%
  distinct()

missing_coms <- nla_comid %>%
  filter(is.na(COMID)) %>%
  select(!COMID) %>%
  left_join(unique_nla_int, by = "UNIQUE_ID")

nla_all_coms <- nla_comid %>%
  filter(is.finite(COMID)) %>%
  bind_rows(missing_coms)

# Pivot from wide to long format
# Problem with some ammonium numbers recorded as "< 0.03", which cannot be coerced to a numeric value. Need to replace these values with something so that the column can be coerced to numeric type.
# Stratified column is a character currently and need to be changed to numeric type to convert to long format
nla_long <- nla_all_coms %>%
  mutate(STRATIFIED = ifelse(is.na(STRATIFIED), 0, 1)) %>%
  mutate(AMMONIA_N = as.numeric(replace(AMMONIA_N, AMMONIA_N == "< 0.03", NA))) %>%
  pivot_longer(TEMPERATURE:B_G_DENS, names_to = "ANALYTE", values_to = "RESULT")

# Where PH data was not measured in the lab, retrieve the top-most pH measurement from the profile
ph_profile <- nla_long %>%
  filter(ANALYTE == "PH") %>%
  filter(is.na(RESULT)) %>%
  select(-RESULT) %>%
  left_join(select(all_nla, VISIT_NO:DSGN_CYCLE, PH_FIELD), 
            by = c("UNIQUE_ID", "DSGN_CYCLE", "VISIT_NO")) %>%
  rename(RESULT = PH_FIELD)

# Remove the rows with missing pH data from the lab and add the field data and add the rows with pH data from the profile data
# Remove NA rows for cylindrospermopsin from 2007 and 2012 surveys because this analysis was not run during those years. Want any NA values present in the df to imply that the analysis was run, but result was below the detection limit.
# Add a units columns
nla_long <- nla_long %>%
  filter(!(ANALYTE == "PH" & is.na(RESULT))) %>%
  bind_rows(ph_profile) %>%
  filter(!(ANALYTE == "CYLSPER" & DSGN_CYCLE %in% c(2007, 2012))) %>%
  mutate(UNITS = case_when(ANALYTE == "TEMPERATURE" ~ "DEG_C",
                           ANALYTE == "STRATIFIED" ~ "Y/N_1/0",
                           ANALYTE %in% c("PTL", "MICX", "CYLSPER") ~ "UG/L",
                           ANALYTE == "TURB" ~ "NTU",
                           ANALYTE == "B_G_DENS" ~ "CELLS/ML",
                           ANALYTE == "PH" ~ "STD_UNITS",
                           ANALYTE %in% c("AMMONIA_N", "DOC", "NTL", "NITRATE_N", "CHLA_RESULT") ~ "MG/L"))

# Performing checks for missing data

# Lots of Nitrate data missing. Looks like everything below the reporting limit was listed as 0.005 mg/L in 2007 (RL = 0.02), but in 2012 (RL = 0.02) and 2017 (RL = 0.0025) ND for nitrate is reported as NA. 
nla_long %>%
  filter(ANALYTE == "NITRATE_N" & is.na(RESULT)) %>%
  nrow()

# Extract NLA observation ID information for joining with other data sources
nla_id <- nla_long %>%
  select(SITE_ID:COMID) %>%
  distinct()

nla_long |>
  filter(ANALYTE %in% c("NITRATE_N", "NTL", "PTL")) |>
  group_by(DSGN_CYCLE, ANALYTE) |>
  summarise(mean = mean(na.omit(RESULT)))

```


Add evaporation to inflow ratio data from Emi Fergus and Renee Brooks


```{r}

# Load data
ratio0712 <- read.csv("NLA2007-2012_E-I_HYDRAP.csv")
ratio17   <- read.csv("NLA17_EI_estimates-Final.csv")

# Transform data to long form
ei0712 <- ratio0712 %>%
  select(SITE_ID, VISIT_NO, YEAR, E_I) %>%
  rename(DSGN_CYCLE = YEAR) %>%
  left_join(nla_id, by = c("SITE_ID", "VISIT_NO", "DSGN_CYCLE")) %>%
  rename(RESULT = E_I) %>%
  mutate(UNITS = "RATIO",
         ANALYTE = "EVAP_INFL")

ei17 <- ratio17 %>%
  select(SITE_ID, E_I, VISIT_NO) %>%
  mutate(DSGN_CYCLE = 2017) %>%
  left_join(nla_id, by = c("SITE_ID", "VISIT_NO", "DSGN_CYCLE")) %>%
  rename(RESULT = E_I) %>%
  mutate(UNITS = "RATIO",
         ANALYTE = "EVAP_INFL")
  
# Bind together 2007/2012 with 2017 data
evap_infl <- rbind(ei0712, ei17)

# Rearrange columns
evap_infl <- relocate(evap_infl, SITE_ID, VISIT_NO, UNIQUE_ID, DSGN_CYCLE, DATE_COL, LAT_DD83, LON_DD83, COMID, ANALYTE, RESULT, UNITS)

# From Emi and Renee: NA values mean that E/I was not calculated for some reason. Generally missing data or unrealistic E/I values were removed
# # Investigating missing values
# filter(ratio0712, is.na(E_I))
# filter(ratio17, is.na(E_I))
#
# write.csv(filter(ratio0712, is.na(E_I)), "missing2012-E_I.csv", row.names = F)
# write.csv(filter(ratio17, is.na(E_I)), "missing2017-E_I.csv", row.names = F)
# 
# filter(ei0712, is.na(COMID))
# filter(ei17, is.na(COMID))

# How many of the NLA sites are missing E/I data?
nrow(nla_id) - nrow(evap_infl)
nrow(nla_id) - nrow(ei17) - nrow(ei0712) # Didn't lose anything in the joining process

# There were fewer rows for the E/I data than there exist for the full NLA data. Unclear why, the E/I data includes site revisits and hand selected sites. Cannot identify a systematic reason for their exclusion.

```


Add watershed metrics compiled by Marc Weber for all NLA lakes (in Lakecat and not in Lakecat). Curiously only includes the 2006 and 2011 NLCD. Bringing in the 2016 NLCD from LakeCat.

```{r}
# Watershed metrics for NLA lakes compiled by Marc
wsmet <- read.csv("NLA_WatershedMetrics_NoDuplicates.csv")

# There is a duplicated set of data for one lake (UNIQUE_ID = "NLA_NV_10003") that has two different SITE_IDs for 2007. LakeCat data is identical between the two. Easiest solution is to remove one row of the data. Removing the one with the hand selected SITE_ID
wsmet <- filter(wsmet, SITE_ID != "NLA06608-NV:4")

# Focus on NLCD first for 2007 and 2012 NLA (pairing with 2006 and 2011 NLCD)
nla0712_nlcd <- wsmet %>%
  mutate(dev_ws = case_when(DSGN_CYCLE == 2007 ~ rowSums(select(wsmet, PctUrbOp2006Ws:PctUrbHi2006Ws)),
                            TRUE ~ rowSums(select(wsmet, PctUrbOp2011Ws:PctUrbHi2011Ws))),
         wet_ws = case_when(DSGN_CYCLE == 2007 ~ PctWdWet2006Ws + PctHbWet2006Ws,
                            TRUE ~ PctWdWet2011Ws + PctHbWet2011Ws),
         nondev_ws = case_when(DSGN_CYCLE == 2007 ~ rowSums(select(wsmet, c(PctOw2006Ws, PctIce2006Ws, PctBl2006Ws:PctGrs2006Ws))),
                               TRUE ~ rowSums(select(wsmet, c(PctOw2011Ws, PctIce2011Ws, PctBl2011Ws:PctGrs2011Ws)))),
         agr_ws = case_when(DSGN_CYCLE == 2007 ~ PctHay2006Ws + PctCrop2006Ws,
                            TRUE ~ PctHay2011Ws + PctCrop2011Ws)) |>
  filter(wsmet$DSGN_CYCLE != 2017) |>
  select(UNIQUE_ID, DSGN_CYCLE, ends_with("_ws")) %>%
  right_join(filter(nla_id, DSGN_CYCLE != 2017), by = c("UNIQUE_ID", "DSGN_CYCLE"))

# Next focus on 2017 NLA to pair with 2016 NLCD data
# Load in NLCD 2016 data
nlcd16 <- readr::read_csv("LakeCat_NLCD2016.csv")

# Find COMIDS for 2017 NLA
nla_id17 <- nla_id |>
  filter(DSGN_CYCLE == 2017) |>
  pull(COMID) |>
  unique()

# Join with the NLA lake COMIDs included in the analysis
# This is losing about 85 lakes. Are these not in lakecat?
nla17_nlcd <- nlcd16 %>%
  mutate(agr_ws = PctHay2016Ws + PctCrop2016Ws,
         dev_ws = rowSums(select(nlcd16, PctUrbOp2016Ws:PctUrbHi2016Ws)),
         nondev_ws = rowSums(select(nlcd16, c(PctOw2016Ws, PctIce2016Ws, PctBl2016Ws:PctGrs2016Ws))),
         wet_ws = PctWdWet2016Ws + PctHbWet2016Ws) %>%
  filter(COMID %in% nla_id17) %>%
  select(COMID, ends_with("_ws")) |>
  right_join(filter(nla_id, DSGN_CYCLE == 2017), by = "COMID")

# How many lakes are missing data from LakeCat for 2017?
length(nla_id17) - nrow(filter(nlcd16, COMID %in% nla_id17))

# What are the COMIDS of the missing lakes? 
nla_id17[!(nla_id17 %in% nlcd16$COMID)]

# Bind together 2007/2012 and 2017 NLA-NLCD data
nla_nlcd <- nla0712_nlcd |>
  bind_rows(nla17_nlcd) |>
  select(UNIQUE_ID, DSGN_CYCLE, VISIT_NO, ends_with("_ws"))

# Next pull in other relevant watershed characteristics
ws_char <- wsmet |>
  select(UNIQUE_ID, DSGN_CYCLE, lakemorpho_depth, RunoffWs, Precip_Minus_EVTWs, WWTPWs, Precip8110Ws, Tmean8110Ws, ElevWs, SlopeWs) %>%
  right_join(nla_id, by = c("UNIQUE_ID", "DSGN_CYCLE"))

# Combine NLCD with other watershed characteristics
ws_char <- right_join(ws_char, nla_nlcd, by = c("UNIQUE_ID", "DSGN_CYCLE", "VISIT_NO"))

```


Pull in 2016 NLCD data for watersheds missing from LakeCat for 2017 NLA


```{r}
# 2016 NLCD data
rast <- read_stars('O:/PRIV/CPHEA/PESD/COR/CORFILES/Geospatial_Library_Projects/StreamCat/LandscapeRasters/QAComplete/nlcd_2006_land_cover_l48_20210604.tif')

# Watershed areas for 2017 NLA lakes
ws17 <- read_sf(dsn='O:/PRIV/CPHEA/PESD/COR/CORFiles/Geospatial_Library_Resource/PHYSICAL/WATERSHEDS/NLA2017_Basins/NLA2017_AllBasins.gdb', layer='NLA17_Basins')

# ID the lakes that were not covered by LakeCat
missing_site_ids <- nla_id |>
  filter(DSGN_CYCLE == 2017) |>
  filter(!(COMID %in% nlcd16$COMID)) |>
  pull(SITE_ID) |>
  unique()

# Still short 14 lakes?
missing_lakes <- filter(ws17, SITE_ID %in% missing_site_ids)

# Ensure they are in the same projection
missing_lakes <- st_transform(missing_lakes, st_crs(rast))

# Try renaming the column holding the geometry
missing_lakes <- rename(missing_lakes, geometry = Shape)

# Check graphically that there is data within the basin
# i <- 81
# lk_pix <- st_crop(rast, missing_lakes[i,], as_points = FALSE)
# lk_pix <- st_as_stars(lk_pix)
# plot(missing_lakes[i,]$geometry)
# plot(lk_pix, add = T)
# plot(missing_lakes[i,]$geometry, add = T)

# Issues with basin for COMID 133250246 SITE_ID NLA17_KS_10011. Some kind of weird multisurface geometry that refuses to be transformed into a multipolygon. Removing for now.
missing_lakes <- filter(missing_lakes, !SITE_ID == "NLA17_KS-10011")

# Extract NLCD for each missing watershed area
missing_nlcd <- do.call(rbind, lapply(c(1:nrow(missing_lakes)), function(i){
  print(i)
  
  # Polygon of the missing lake's watershed area
  lake <- missing_lakes[i,]
  
  # Crop the NLCD 2016 data, as_points = F to include all pixels intersecting the basin shape
  ws_pix <- st_crop(rast, missing_lakes[i,], as_points = FALSE)
  
  # Convert from stars proxy to stars object and create table of NLCD pixel counts within watershed boundary
  nlcd_extract <- table(st_as_stars(ws_pix))

  # Preemptively rename the attributes to match that in LakeCat
  names(nlcd_extract) <- c("Unclassified", "PctOw2016Ws", "PctIce2016Ws", "PctUrbOp2016Ws", "PctUrbLo2016Ws", "PctUrbMd2016Ws", "PctUrbHi2016Ws", "PctBl2016Ws", "PctDecid2016Ws", "PctConif2016Ws", "PctMxFst2016Ws", "PctShrb2016Ws", "PctGrs2016Ws", "PctHay2016Ws", "PctCrop2016Ws", "PctWdWet2016Ws", "PctHbWet2016Ws")
  
  # Calculate total area included in the basin
  total_area = sum(nlcd_extract) * 900
  
  # Calculate proportions of each land cover type and convert to wide format to match LakeCat
  nlcd_wide <- nlcd_extract |>
    as.data.frame() |>
    rename(cover_type = 1) |>
    filter(!cover_type == "Unclassified") |>
    mutate(props = (Freq * 900) / total_area * 100, .keep = "unused") |>
    tidyr::pivot_wider(names_from = cover_type, values_from = props) |>
    mutate(SITE_ID = missing_lakes$SITE_ID[i],
           COMID = missing_lakes$COMID[i], .before = 1) 
  
  return(nlcd_wide)
}))

# Sum into categories and join with NLA ID
# Note: one of the sites was visited twice in 2017, so this produces a dataframe of nrow(missing_nlcd) + 1
# Note: Two sites ("NLA17_MO-HP005", "NLA17_TX-HP005") are hand picked sites and lack COMIDs (NLA COMID = 0)
missing_nlcd_sums <- missing_nlcd %>%
  mutate(agr_ws = PctHay2016Ws + PctCrop2016Ws,
         dev_ws = rowSums(select(missing_nlcd, PctUrbOp2016Ws:PctUrbHi2016Ws)),
         nondev_ws = rowSums(select(missing_nlcd, c(PctOw2016Ws, PctIce2016Ws, PctBl2016Ws:PctGrs2016Ws))),
         wet_ws = PctWdWet2016Ws + PctHbWet2016Ws) |>
  select(SITE_ID, ends_with("_ws")) |>
  left_join(filter(nla_id, DSGN_CYCLE == 2017), by = c("SITE_ID"))

# There's one more row than expected due to two site visits for one of the lakes
dup_site <- nlcd_sums$SITE_ID[duplicated(nlcd_sums$SITE_ID)]
filter(nlcd_sums, SITE_ID == dup_site)

# Two sites ("NLA17_MO-HP005", "NLA17_TX-HP005") are hand picked sites and lack COMIDs (NLA COMID = 0)
filter(nlcd_sums, SITE_ID %in% c("NLA17_MO-HP005", "NLA17_TX-HP005"))
filter(missing_lakes, SITE_ID %in% c("NLA17_MO-HP005", "NLA17_TX-HP005"))
filter(nla_id, SITE_ID %in% c("NLA17_MO-HP005", "NLA17_TX-HP005"))



```


Add in nutrient inventory data compiled by Robert Sabo and others


```{r}
# Note that nutrient inventory data is in units of kg N/yr or kg P/yr

# Read in the 2007 data
nni2007 <- read.csv("NLA07_NutrientInventory.csv")

# Read in the 2012 data
nni2012 <- read.csv("NLA12_NutrientInventory.csv")

# Select the relevant columns from each and join with NLA ID info to get unique IDs
extractor <- function(x, year){
  x |> 
    select(SITE_ID, VISIT_NO, contains(c("Total_Inputs", "Surplus"))) |>
    left_join(filter(nla_id, DSGN_CYCLE == year), by = c("SITE_ID", "VISIT_NO"))
}

nni07 <- extractor(nni2007, 2007)
nni12 <- extractor(nni2012, 2012)

nni0712 <- bind_rows(nni07, nni12)

# Note that the same lake can have (usually small) differences in the nutrient inputs the same NNI year in the 2007 and 2012 NLA files. This is because the NNI was downscaled to each lake watershed using the corresponding NLCD data for the NLA survey (2007 NLA = 2006 NLCD, 2012 NLA = 2011 NLCD). Because of slight changes in the relevant land use classes in the two NLCDs, there are differences in the derived inventories based on this.
nni_mean <- nni0712 |>
  mutate(N_Total_Inputs = rowMeans(select(nni0712, starts_with('N_Total_Inputs')), na.rm = TRUE),
         N_Surplus = rowMeans(select(nni0712, starts_with('N_Surplus')), na.rm = TRUE),
         P_Surplus = rowMeans(select(nni0712, starts_with('P_Surplus')), na.rm = TRUE)) |>
  filter(VISIT_NO == 1) |>
  group_by(UNIQUE_ID) |>
  summarise(N_Total_Inputs = mean(N_Total_Inputs, na.rm = T),
            N_Surplus = mean(N_Surplus, na.rm = T),
            P_Surplus = mean(P_Surplus, na.rm = T)) |>
  drop_na(N_Total_Inputs, N_Surplus, P_Surplus)

```


Compile together NLA, NLA+, LakeCat, and Nutrient Inventory data

```{r}
# Convert from long to wide format
habs_comb <- nla_long %>%
  bind_rows(evap_infl) %>%
  select(!UNITS) %>%
  pivot_wider(names_from = ANALYTE, values_from = RESULT, values_fill = NA) %>%
  left_join(select(ws_char, UNIQUE_ID:SlopeWs, dev_ws:agr_ws), by = c("UNIQUE_ID", "DSGN_CYCLE")) %>%
  left_join(nni_mean, by = "UNIQUE_ID") |>
  filter(!is.na(UNIQUE_ID)) |>
  mutate(DSGN_CYCLE = factor(DSGN_CYCLE),
         UNIQUE_ID = factor(UNIQUE_ID))

# Convert to a sf object
habs_nars_all <- sf::st_as_sf(habs_comb, coords = c('LON_DD83', 'LAT_DD83'), crs = 4269, remove = FALSE)

# Transform to an equal area projection so that x and y distances are equivalent
habs <- sf::st_transform(habs_nars_all, crs = 5070)

# Save the data as part of the package
# usethis::use_data(habs, overwrite = T)

```


Figures for presentations 


```{r}
# Presentations for CPHEA managers call, JASM, and PESD Science Seminar

# Get examples of climate, watershed, lake morphology, and water chemistry to illustrate how these data sources will be used in the conceptualized modeling project

# Climate data: MAT
mat <- read.csv("PRISM_1981_2010.csv")

# Watershed: Percent agricultural cover
# Lake morphology: Lake depth
wsmet <- read.csv("NLA_WatershedMetrics.csv")

# Response variables: Cyanobacteria cell abundance and microcystin concentration
# Water chemistry: Total nitrogen
pres <- nla_long %>%
  filter(ANALYTE %in% c("MICX", "B_G_DENS", "NTL")) %>%
  pivot_wider(id_cols = SITE_ID:COMID, names_from = ANALYTE, values_from = RESULT)

# Join mean temperature data with HABs data
pres <- mat %>%
  select(COMID, Tmean8110Ws) %>%
  right_join(pres, by = "COMID")

# Sum crop and hay percents to get agriculture
# Join to the HABs data
# Note there's a problem here with duplicated 
wsag <- wsmet %>%
  select(COMID, DSGN_CYCLE, lakemorpho_depth, PctHay2006Ws, PctCrop2006Ws, PctDecid2006Ws, PctConif2006Ws, PctMxFst2006Ws) %>%
  distinct() %>%
  mutate(PctAg2006Ws = PctHay2006Ws + PctCrop2006Ws,
         PctFst2006Ws = PctDecid2006Ws + PctConif2006Ws + PctMxFst2006Ws,
         .keep = "unused") %>%
  # This is a temporary fix to get data compiled. Will need to ask Marc about how to deal with repeat COMIDs
  distinct(COMID, DSGN_CYCLE, .keep_all = T)
  
# Join to presentation data
pres <- left_join(pres, wsag, by = c("COMID", "DSGN_CYCLE"))

expl_var <- "NTL"; xlabel <- 'Total Nitrogen (mg N/L)'; file_name <- "cyano_tn_biplot"
expl_var <- "Tmean8110Ws"; xlabel <- 'Mean Annual Temp (deg C)'; file_name <- "cyano_MAT_biplot"
expl_var <- "lakemorpho_depth"; xlabel <- 'Max Lake Depth (m)'; file_name <- "cyano_lakedepth_biplot"
expl_var <- "PctAg2006Ws"; xlabel <- 'Watershed Agriculture Land Use (%)'; file_name <- "cyano_ag_biplot"
expl_var <- "PctFst2006Ws"; xlabel <- 'Watershed Forest Cover (%)'; file_name <- "cyano_forest_biplot"



# Plot data
rev_biplots <- function(expl_var, file_name){
  
  ylabel <- 'Cyanobacteria (cells/mL + 1)'
  
  # Transform cyanobacteria data by adding 1 to make log transformation possible
  plot_data <- mutate(pres, tran_cyano = B_G_DENS + 1)
  
  # Order row my microcystin
  plot_data <- plot_data[order(plot_data$MICX, na.last = F),]
  
  plot_var <- plot_data[,match(expl_var, colnames(plot_data))]
  
  # For the Ag, need to add a very small number to the data to 
  # plot_var <- plot_var + 0.00001
  
  # For the Forest, need to add a very small number to the data to 
  # plot_var <- plot_var + 0.0001
  
  ggplot(data = plot_data, aes(x = plot_var, y = tran_cyano)) +
    geom_point(aes(color = MICX), stroke = .7, alpha = 0.7, size = 3) +
    # scale_x_continuous(trans = 'log10',
    #                    breaks = trans_breaks("log10", function(x) 10^x),
    #                    labels = trans_format("log10", math_format(10^.x))) +
    scale_y_continuous(trans = 'log10',
                       breaks = trans_breaks("log10", function(x) 10^x),
                       labels = trans_format("log10", math_format(10^.x))) +
    xlab(xlabel) +
    ylab(ylabel) +
    scale_color_gradient(low = "gold", high = 'red',
                         name = "Microcystin (ug/L)",
                         limits = c(0.1, 230),
                         trans = 'log10', 
                         breaks = c(0.1, 1, 10, 100),
                         labels = c("0.1", "1.0", "10", "100")) +
    theme_bw() +
    theme(panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank())
  
  # Original size
  ggsave(paste0("./figures/",file_name,".png"), height = 3, width = 7, units = 'in', dpi = 600, bg = "transparent")
}




```




```{r, include = FALSE, warning = FALSE}

# Add 1 to cyanobacteria counts so that zero data will appear on log-transformed figures
nla_nlcd <- nla_nlcd %>%
  mutate(RESULT = ifelse(ANALYTE == "B_G_DENS", RESULT + 1, RESULT))

# Make a massive number of graphs
# ggplot(nla_nlcd, aes(y = RESULT, x = PctOw2016Cat)) +
#   geom_point(pch = 21, alpha = 0.5) +
#   facet_wrap(. ~ ANALYTE, scales = "free_y",  ncol = 4) +
#    scale_y_continuous(trans = 'log10',
#                        breaks = trans_breaks("log10", function(x) 10^x),
#                        labels = trans_format("log10", math_format(10^.x)))

# Make function
plot_nla_nlcd <- function(xvar, name){
  ggplot(nla_nlcd, aes(y = RESULT, x = xvar)) +
    geom_point(pch = 19, alpha = 0.5) +
    facet_wrap(. ~ ANALYTE, scales = "free_y",  ncol = 4) +
    xlab(name) +
    scale_y_continuous(trans = 'log10',
                       breaks = trans_breaks("log10", function(x) 10^x),
                       labels = trans_format("log10", math_format(10^.x)))
}

names <- colnames(nla_nlcd[c(8:39)])

nlcd_plots <- lapply(names, function(name){
  # Get col num
  colnum <- match(name, colnames(nla_nlcd))
  
  # Get column data
  x_var <- nla_nlcd[,colnum]
  
  # Make fig
  plot_nla_nlcd(x_var, name)
  
})

```

```{r, echo = FALSE, warning = FALSE, fig.width = 10, fig.height = 2 * length(nlcd_plots)}

ggarrange(plotlist = nlcd_plots, ncol = 1)



```

---
title: "drivers_analysis"
author: "Handler"
date: '2022-11-21'
output: html_document
editor_options: 
  chunk_output_type: console
---

Based on data compiled in drivers_compilation.Rmd

```{r setup, include=FALSE}
library(usethis)
library(devtools)
library(spmodel)
library(sf)
library(mapview)
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)

# Load all data and functions from HABsDrivers package
load_all()

# Detailed guide to spmodel
# https://usepa.github.io/spmodel/articles/guide.html
```

Calculate the chlorophyll a associated with cyanobacteria

```{r}
habs <- mutate(habs, CHLA_CYANO = CHLA_RESULT * BG_BIOVOL/PHYT_BIOVOL, .after = CHLA_RESULT)
```


Examine the predictor data for colinear variables
- TN and DOC
- Precip8110Ws and Precip_Minus_EVTWs and RunoffWs
- nondev_ws and agr_ws
- SlopeWs and ElevWs
- N_Total_Inputs, N_Surplus, P_Surplus (r > 0.90)


```{r}
cormat_pred <- cor(st_drop_geometry(select(habs, TEMPERATURE:P_Surplus)), use = "pairwise.complete.obs")

# cormat_pred <- cor(st_drop_geometry(select(habs, NTL, PTL, DOC, TURB, PH, TEMPERATURE, dev_ws, wet_ws, nondev_ws, agr_ws, lakemorpho_depth, Precip8110Ws, RunoffWs, Tmean8110Ws, ElevWs, SlopeWs, N_Surplus)), use = "pairwise.complete.obs")

corrplot.mixed(cormat_pred, upper = "ellipse", diag = "n", tl.pos = "lt", tl.col = "black", tl.srt = 45)

```


How much of the variation in the response variables is potentially explainable based on the spatial versus temporal variation?


```{r}
# Assess spatial (signal) to temporal (noise) in response variables
do.call(rbind, lapply(c("MICX", "B_G_DENS", "CHLA_RESULT", "CHLA_CYANO"), function(x){
  form <- as.formula(paste(x, " ~ 1 + (1|SITE_ID)"))
  df   <- habs
  
  if(x == "MICX"){
    df <- drop_na(df, MICX)
  }
  
  if(x == "B_G_DENS"){
    form <- as.formula(paste("log10(", x, "+ 1) ~ 1 + (1|SITE_ID)"))
  }
  
  if(x == "CHLA_RESULT"){
    form <- as.formula(paste("log10(", x, ") ~ 1 + (1|SITE_ID)"))
  }
  
  if(x == "CHLA_CYANO"){
    form <- as.formula(paste("log10(", x, ") ~ 1 + (1|SITE_ID)"))
  }
  
  mod         <- lme4::lmer(form, data = df, REML = T, verbose = F)
  site.var    <- as.numeric(lme4::VarCorr(mod)) 
  visit.var   <- attr(lme4::VarCorr(mod), "sc")^2
  sn          <- site.var / visit.var
  maxR2       <- sn/(sn + 1)
  return(data.frame(variable = x, maxR2))
}))


```


Experiment with some spatial models


```{r}
# Function that creates the formula. When the response variable is cyanobacteria cell counts, add 1 and log-transform.
create_formula <- function(dep_var, ind_vars, log = F){
  # Log = F
  as.formula(paste(dep_var, " ~ ", paste(ind_vars, collapse = " + ")))
  # Log = T
  if(log == T){
    as.formula(paste("log10(", dep_var, "+ 1) ~ ", paste(ind_vars, collapse = " + ")))
  }
}

# Create variable groups
all_vars     <- colnames(habs)[match("TEMPERATURE", colnames(habs)):match("P_Surplus", colnames(habs))]
dep_vars     <- c("B_G_DENS", "MICX", "CYLSPER", "CHLA_RESULT", "CHLA_CYANO")
all_ind_vars <- all_vars[!all_vars %in% dep_vars]

# Dropping non-developed area of the watershed due to issues of colinearity with developed area. Also dropping nitrate data since it is missing for a large number of observations.
all_ind_vars <- all_ind_vars[!all_ind_vars %in% c("nondev_ws", "NITRATE_N")]

# NLA water variables
nla_ind_vars <- c("TEMPERATURE", "STRATIFIED", "AMMONIA_N", "DOC", "NTL", "PTL", "TURB", "NITRATE_N", "PH", "EVAP_INFL")
  
# All non-NLA variables
non_nla_vars <- all_ind_vars[!all_ind_vars %in% nla_ind_vars]

dep_var <- "B_G_DENS"

all_no_re_no_sp <- splm(create_formula(dep_var, all_ind_vars, log = T),
                     data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars)),
                     spcov_type = "none", 
                     local = TRUE)

all_no_re <- splm(create_formula(dep_var, all_ind_vars, log = T),
                     data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars)),
                     spcov_type = "exponential", 
                     local = TRUE)

all_re_site <- splm(create_formula(dep_var, all_ind_vars, log = T),
                         data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars)),
                         spcov_type = "exponential",
                         random = ~  UNIQUE_ID,
                         local = TRUE)

all_re_year <- splm(create_formula(dep_var, all_ind_vars, log = T),
                         data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars)),
                         spcov_type = "exponential",
                         random = ~  DSGN_CYCLE,
                         local = TRUE)

all_unnest_re <- splm(create_formula(dep_var, all_ind_vars, log = T),
                       data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars)),
                       spcov_type = "exponential",
                       random = ~ DSGN_CYCLE + UNIQUE_ID,
                       local = TRUE)

all_nest_re <- splm(create_formula(dep_var, all_ind_vars, log = T),
                       data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars)),
                       spcov_type = "exponential",
                       random = ~ (DSGN_CYCLE / UNIQUE_ID),
                       local = TRUE)

# Compare model with no spatial component, with spatial component, random effect for site alone, random effect for year along, random effect for site and year unnested, random effect for site nested within year.
# How does the AIC and R2 change with these model configurations
glances(all_no_re_no_sp,
        all_no_re,
        all_re_site,
        all_re_year,
        all_unnest_re,
        all_nest_re)

# Unnested random effects performed the best
# Model with random effect for year (no site random effect) performed almost as well with respect to AIC with slightly higher pseudo R2

# Explore 

# Would be good to investigate this with Robert's data as well since that includes a bunch of data about the weather for that specific year


nolake_no_re <- splm(create_formula(dep_var, non_nla_vars, log = T),
                     data = drop_na(habs, all_of(dep_var), all_of(non_nla_vars)),
                     spcov_type = "exponential", 
                     local = TRUE)

nolake_unnest_re <- splm(create_formula(dep_var, non_nla_vars, log = T),
                         data = drop_na(habs, all_of(dep_var), all_of(non_nla_vars)),
                         spcov_type = "exponential",
                         random = ~ DSGN_CYCLE + UNIQUE_ID,
                         local = TRUE)

nolake_nest_re <- splm(create_formula(dep_var, non_nla_vars, log = T),
                       data = drop_na(habs, all_of(dep_var), all_of(non_nla_vars)),
                       spcov_type = "exponential",
                       random = ~ (DSGN_CYCLE / UNIQUE_ID),
                       local = TRUE)


# Compare models
glances(nolake_no_re, nolake_unnest_re, nolake_nest_re)

glances(nolake_no_re, 
        nolake_unnest_re, 
        nolake_nest_re,
        all_re_year,
        all_unnest_re,
        all_nest_re)

# Explore specifying a random effect for ecoregion
# First, a model just using total N as an independent variable
ecor_test1 <- splm(create_formula(dep_var, "NTL", log = T),
                         data = drop_na(habs, all_of(dep_var), NTL),
                         spcov_type = "exponential",
                         random = ~ DSGN_CYCLE + UNIQUE_ID,
                         local = TRUE)

# Second, add in the ecoregion and allow slopes to vary by ecoregions
ecor_test2 <- splm(create_formula(dep_var, "NTL", log = T),
                         data = drop_na(habs, all_of(dep_var), NTL),
                         spcov_type = "exponential",
                         random = ~ DSGN_CYCLE + UNIQUE_ID + (NTL | AG_ECO9_NM),
                         local = TRUE)

# Add a few other potential covariates
ecor_test3 <- splm(log10(B_G_DENS + 1) ~ NTL + precip_mean_month,
                         data = drop_na(habs, B_G_DENS, NTL, precip_mean_month),
                         spcov_type = "exponential",
                         random = ~ DSGN_CYCLE + UNIQUE_ID + (NTL | AG_ECO9_NM) + (precip_mean_month | AG_ECO9_NM),
                         local = TRUE)

# How does have a slope for each ecoregion change the model?
glances(ecor_test1, ecor_test2)
varcomp(ecor_test1)
varcomp(ecor_test2)

# Try random forest
# A good tutorial introducing random forest https://uc-r.github.io/random_forests
modRF <- splmRF(create_formula(dep_var, all_ind_vars, log = T),
                data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars)),
                spcov_type = "exponential",
                random = ~ DSGN_CYCLE + UNIQUE_ID,
                # random = ~ DSGN_CYCLE,
                local = TRUE,
                num.trees       = 500,
                mtry            = 7,
                min.node.size   = 3,
                sample.fraction = .8,
                importance = "impurity")

imp <- tibble(var = names(modRF$ranger$variable.importance), var_imp = modRF$ranger$variable.importance)

imp %>% 
  dplyr::arrange(desc(var_imp)) %>%
  # dplyr::top_n(25) %>%
  ggplot(aes(x = reorder(var, var_imp), y = var_imp)) +
  geom_col() +
  coord_flip() +
  xlab("Variable") + ylab("Variable Importance")
ggsave("./figures/Variable importance_spMod_NLA'07,'12,'17_all.png", dpi = 300, width = 5, height = 5)

# Examine if the error stabilizes with the number of trees
m1 <- randomForest::randomForest(
  formula = create_formula(dep_var, all_ind_vars, log = T),
  data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars))
)
plot(m1)

# Tune the mTry parameter
na_free <- drop_na(habs, all_of(dep_var), all_of(all_ind_vars))

m2 <- randomForest::tuneRF(
  x          = st_drop_geometry(na_free[all_ind_vars]),
  y          = log10(na_free$B_G_DENS + 1),
  ntreeTry   = 500,
  mtryStart  = 4,
  stepFactor = 1.5,
  improve    = 0.01,
  trace      = FALSE      # to not show real-time progress 
)

# hyperparameter grid search
hyper_grid <- expand.grid(
  mtry       = seq(4, 8, by = 1),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE   = 0
)

for(i in 1:nrow(hyper_grid)) {
  
  # train model
  model <- ranger::ranger(
    formula         = create_formula(dep_var, all_ind_vars, log = T),
    data            = st_drop_geometry(drop_na(habs, all_of(dep_var), all_of(all_ind_vars))), 
    num.trees       = 500,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed            = 123
  )
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

# Which set of parameters yields the lowest OOB RMSE?
hyper_grid %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(10)

# Check expected error for optimal set of hyperparameters
OOB_RMSE <- vector(mode = "numeric", length = 100)

for(i in seq_along(OOB_RMSE)) {

  optimal_ranger <- ranger::ranger(
    formula         = create_formula(dep_var, all_ind_vars, log = T),
    data            = st_drop_geometry(drop_na(habs, all_of(dep_var), all_of(all_ind_vars))), 
    num.trees       = 500,
    mtry            = 8,
    min.node.size   = 9,
    sample.fraction = .8,
    importance      = 'impurity'
  )
  
  OOB_RMSE[i] <- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)

imp <- tibble(var = names(optimal_ranger$variable.importance), var_imp = optimal_ranger$variable.importance)

imp %>% 
  dplyr::arrange(desc(var_imp)) %>%
  ggplot(aes(x = reorder(var, var_imp), y = var_imp)) +
  geom_col() +
  coord_flip() +
  xlab("Variable") + ylab("Variable Importance") +
  ggtitle("NLA '07,'12,'17_All Variables")

ggsave("./figures/Variable importance_NLA'07,'12,'17_all.png", dpi = 300, width = 5, height = 5)
  
# Check model performance by loocv
# These take a very long time to run. Running the nested model went for 1.25 hours and didn't finish. Should maybe explore doing a k-folds cross validation.
# loocv(inlake_nest_re, local = T) # 0.8567984
# loocv(nolake_nest_re, local = T)

# Split data into training and predictor sets
habs_pred <- predict(inlake_nest_re, newdata = habs, local = TRUE)

# Visualize the standardized residuals
re_nested_resid <- augment(inlake_nest_re)
mapview(re_nested_resid, zcol = ".std.resid", legend = TRUE)

# Filter to just the big residuals
tmp <- filter(re_nested_resid, .std.resid < -2 | .std.resid > 2)
mapview(tmp, zcol = ".std.resid", legend = TRUE)
# Seem fairly randomly distributed across the country

mapview(re_nested_resid, zcol = ".std.resid", at = c(-5.4, -2, 2.6), legend = TRUE)

ggplot(data = re_nested_resid, aes(x = .fitted, y = `log10(B_G_DENS + 1)`)) + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1)

ggplot(data = re_nested_resid, aes(x = .std.resid)) + 
  geom_histogram() + 

summary(mod_re_nested)
summary(mod_re_unnested)

plot(mod_re_nested)
plot(mod_re_unnested)

# Check that the spatial model performs better than a non-spatial model
inlake_nonsp <- splm(create_formula(dep_var, in_lake_vars, log = T),
                     data = drop_na(habs, all_of(dep_var), all_of(in_lake_vars)),
                     spcov_type = "none",
                     random = ~ (DSGN_CYCLE / UNIQUE_ID),
                     local = TRUE)

# Compare spatial and non-spatial models with nested random effects
glances(inlake_nonsp, inlake_nest_re)
# Hmmm, the non-spatial model performs just as well (maybe a little better) than the spatial model

# Is it because there are so many independent variables?
basic_spat <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "exponential", local = TRUE) 

basic_nonsp <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "none", local = TRUE)

glances(basic_nonsp, basic_spat)
# Reducing the number of independent variables doesn't change the answer. The spatial and non-spatial forms of the model perform about the same

# Try a few different spatial convariance types
gaussian <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "gaussian", local = TRUE) 

sphere <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "spherical", local = TRUE) 

triangular <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "triangular", local = TRUE)

circular <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "circular", local = TRUE)

cubic <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "cubic", local = TRUE)

glances(basic_nonsp, basic_spat, gaussian, sphere, triangular, circular, cubic)
# Conclusion: Exponential and cubic covariance structure perform the same and only slightly better than other form.

summary(mod_re_nested)
summary(mod_re_unnested)

tidy(mod_re_nested)
glance(mod_re_nested)
plot(mod_re_nested)
plot(mod_re_unnested)

```


Try random forest with large collection of data from Robert Sabo


```{r}
# All data from Robert (untransformed)
nni <- readr::read_csv("NLA07-12_NutrientInventory_Data.csv")

# Have to break out the month and day of the compiled HABs data
comp <- habs |>
  mutate(Month = as.numeric(format(as.Date(DATE_COL, format = "%m/%d/%Y"), "%m")),
         Day = as.numeric(format(as.Date(DATE_COL, format = "%m/%d/%Y"), "%d")),
         Year = as.numeric(as.character(DSGN_CYCLE))) |>
  # select(SITE_ID, Year, Month, Day, B_G_DENS, TEMPERATURE, DOC, TURB, PH, EVAP_INFL, lakemorpho_depth, ElevWs, SlopeWs) 
  # Remove redundant variables
  select(-NTL, -PTL, -RunoffWs, -LAT_DD83, -LON_DD83, -DATE_COL, -DSGN_CYCLE, -VISIT_NO, -COMID, -MICX, -CHLA_RESULT, -CYLSPER, -UNIQUE_ID)

# Merge with cyanobacteria counts
comp <- left_join(nni, comp, by = c("SITE_ID", "Year", "Month", "Day"))

# Create a vector of the independent variables
nni_vars <- colnames(comp)[c(9:91,93:(length(colnames(comp))-1))] #

# Examine if the error stabilizes with the number of trees
m1 <- randomForest::randomForest(
  formula = create_formula(dep_var, nni_vars, log = T),
  data = drop_na(comp, all_of(dep_var), all_of(nni_vars))
)
plot(m1)

# Tune the mTry parameter
na_free <- drop_na(comp, all_of(dep_var), all_of(nni_vars))

m2 <- randomForest::tuneRF(
  x          = st_drop_geometry(na_free[nni_vars]),
  y          = log10(na_free$B_G_DENS + 1),
  ntreeTry   = 500,
  mtryStart  = 4,
  stepFactor = 1.5,
  improve    = 0.01,
  trace      = FALSE      # to not show real-time progress 
)

# hyperparameter grid search
hyper_grid <- expand.grid(
  mtry       = seq(25, 35, by = 2),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE   = 0
)

for(i in 1:nrow(hyper_grid)) {
  
  # train model
  model <- ranger::ranger(
    formula         = create_formula(dep_var, nni_vars, log = T),
    data            = st_drop_geometry(drop_na(comp, all_of(dep_var), all_of(nni_vars))), 
    num.trees       = 500,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed            = 123,
    num.threads     = 8
  )
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

# Which set of parameters yields the lowest OOB RMSE?
hyper_grid %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(15)

# Check expected error for optimal set of hyperparameters
OOB_RMSE <- vector(mode = "numeric", length = 100)

for(i in seq_along(OOB_RMSE)) {

  optimal_ranger <- ranger::ranger(
    formula         = create_formula(dep_var, nni_vars, log = T),
    data            = st_drop_geometry(drop_na(comp, all_of(dep_var), all_of(nni_vars))), 
    num.trees       = 500,
    mtry            = 29,
    min.node.size   = 3,
    sample.fraction = .8,
    num.threads     = 8, 
    importance      = 'impurity'
  )
  
  OOB_RMSE[i] <- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)

optimal_ranger <- ranger::ranger(
  formula         = create_formula(dep_var, nni_vars, log = T),
  data            = drop_na(comp, all_of(dep_var), all_of(nni_vars)), 
  num.trees       = 500,
  mtry            = 29,
  min.node.size   = 3,
  sample.fraction = .8,
  num.threads     = 8, 
  importance      = 'impurity')

imp <- tibble(var = names(optimal_ranger$variable.importance), var_imp = optimal_ranger$variable.importance)

imp %>% 
  dplyr::arrange(desc(var_imp)) %>%
  dplyr::top_n(25) %>%
  ggplot(aes(x = reorder(var, var_imp), y = var_imp)) +
  geom_col() +
  coord_flip() +
  xlab("Variable") + ylab("Variable Importance") +
  ggtitle("NLA '07,'12_All Variables")

ggsave("./figures/Variable importance_NLA'07,'12_all.png", dpi = 300, width = 5, height = 5)

########################################################

# Have to break out the month and day of the compiled HABs data
comp <- habs |>
  mutate(Month = as.numeric(format(as.Date(DATE_COL, format = "%m/%d/%Y"), "%m")),
         Day = as.numeric(format(as.Date(DATE_COL, format = "%m/%d/%Y"), "%d")),
         Year = as.numeric(as.character(DSGN_CYCLE))) |>
  select(-NTL, -PTL, -RunoffWs) |>
  right_join(nni, by = c("SITE_ID", "Year", "Month", "Day")) # |>
  # filter(!duplicated(UNIQUE_ID)) # If running with unique observations

# Investigate how the year random effect interacts with some data that is specific to the sample year
nni_sub <- c(all_vars, "LSTAnomaly_YrMean", "NPP_YrMean", "Precip_YrMean", "Tmean_YrMean")

nni_no_re_no_sp <- splm(create_formula(dep_var, nni_sub, log = T),
                     data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                     spcov_type = "none", 
                     local = TRUE)

nni_no_re <- splm(create_formula(dep_var, nni_sub, log = T),
                     data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                     spcov_type = "exponential", 
                     local = TRUE)

nni_re_site <- splm(create_formula(dep_var, nni_sub, log = T),
                         data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                         spcov_type = "exponential",
                         random = ~  UNIQUE_ID,
                         local = TRUE)

nni_re_year <- splm(create_formula(dep_var, nni_sub, log = T),
                         data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                         spcov_type = "exponential",
                         random = ~  DSGN_CYCLE,
                         local = TRUE)

nni_unnest_re <- splm(create_formula(dep_var, nni_sub, log = T),
                       data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                       spcov_type = "exponential",
                       random = ~ DSGN_CYCLE + UNIQUE_ID,
                       local = TRUE)

nni_nest_re <- splm(create_formula(dep_var, nni_sub, log = T),
                       data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                       spcov_type = "exponential",
                       random = ~ (DSGN_CYCLE / UNIQUE_ID),
                       local = TRUE)

# Compare model with no spatial component, with spatial component, random effect for site alone, random effect for year along, random effect for site and year unnested, random effect for site nested within year.
# How does the AIC and R2 change with these model configurations
glances(nni_no_re_no_sp,
        nni_no_re,
        nni_re_site,
        nni_re_year,
        nni_unnest_re,
        nni_nest_re)
# Nested random effects had the lowest AIC and R2 followed by unnested random effects
# About the same amount of variation is explained with no random effects

optimal_ranger <- ranger::ranger(
  formula         = create_formula(dep_var, nni_sub, log = T),
  data            = drop_na(comp, all_of(dep_var), all_of(nni_sub)), 
  num.trees       = 500,
  mtry            = 8,
  min.node.size   = 3,
  sample.fraction = .8,
  num.threads     = 8, 
  importance      = 'impurity')

imp <- tibble(var = names(optimal_ranger$variable.importance), var_imp = optimal_ranger$variable.importance)

imp %>% 
  dplyr::arrange(desc(var_imp)) %>%
  dplyr::top_n(25) %>%
  ggplot(aes(x = reorder(var, var_imp), y = var_imp)) +
  geom_col() +
  coord_flip() +
  xlab("Variable") + ylab("Variable Importance") +
  ggtitle("NLA '07,'12_All Variables")

ggsave("./figures/Variable importance_NLA'07,'12_with annual.png", dpi = 300, width = 5, height = 5)


```



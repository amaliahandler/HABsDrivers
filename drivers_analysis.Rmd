---
title: "drivers_analysis"
author: "Handler"
date: '2022-11-21'
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(usethis)
library(devtools)
library(spmodel)
library(sf)
library(mapview)
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)

# Load all data and functions from HABsDrivers package
load_all()

# Detailed guide to spmodel
# https://usepa.github.io/spmodel/articles/guide.html
```


Examine the predictor data for colinear variables
- TN and DOC
- Precip8110Ws and Precip_Minus_EVTWs and RunoffWs
- nondev_ws and agr_ws
- SlopeWs and ElevWs
- N_Total_Inputs, N_Surplus, P_Surplus (r > 0.90)


```{r}
cormat_pred <- cor(st_drop_geometry(select(habs, TEMPERATURE:PH, EVAP_INFL:P_Surplus)), use = "pairwise.complete.obs")

cormat_pred <- cor(st_drop_geometry(select(habs, NTL, PTL, DOC, TURB, PH, TEMPERATURE, dev_ws, wet_ws, nondev_ws, agr_ws, lakemorpho_depth, Precip8110Ws, RunoffWs, Tmean8110Ws, ElevWs, SlopeWs, N_Surplus)), use = "pairwise.complete.obs")

corrplot.mixed(cormat_pred, upper = "ellipse", diag = "n", tl.pos = "lt")

```


How much of the variation in the response variables is potentially explainable based on the spatial versus temporal variation?


```{r}
# Assess spatial (signal) to temporal (noise) in response variables
do.call(rbind, lapply(c("MICX", "B_G_DENS", "CHLA_RESULT"), function(x){
  form <- as.formula(paste(x, " ~ 1 + (1|SITE_ID)"))
  df   <- habs
  
  if(x == "MICX"){
    df <- drop_na(df, MICX)
  }
  
  if(x == "B_G_DENS"){
    form <- as.formula(paste("log10(", x, "+ 1) ~ 1 + (1|SITE_ID)"))
  }
  
  if(x == "CHLA_RESULT"){
    form <- as.formula(paste("log10(", x, ") ~ 1 + (1|SITE_ID)"))
  }
  
  mod         <- lme4::lmer(form, data = df, REML = T, verbose = F)
  site.var    <- as.numeric(lme4::VarCorr(mod)) 
  visit.var   <- attr(lme4::VarCorr(mod), "sc")^2
  sn          <- site.var / visit.var
  maxR2       <- sn/(sn + 1)
  return(data.frame(variable = x, maxR2))
}))


```


Experiment with some spatial models


```{r}
create_formula <- function(dep_var, ind_vars, log = F){
  # Log = F
  as.formula(paste(dep_var, " ~ ", paste(ind_vars, collapse = " + ")))
  # Log = T
  if(log == T){
    as.formula(paste("log10(", dep_var, "+ 1) ~ ", paste(ind_vars, collapse = " + ")))
  }
}

# The set of in-lake, watershed, and climate variables
in_lake_vars <- c("NTL", "PTL", "DOC", "TURB", "PH", "TURB", "TEMPERATURE", "STRATIFIED", "EVAP_INFL", "lakemorpho_depth", "Precip8110Ws", "RunoffWs", "Tmean8110Ws", "ElevWs", "SlopeWs", "dev_ws", "wet_ws", "agr_ws")

# The set of variables excluding in-lake data
no_lake_vars <- c("lakemorpho_depth", "Precip8110Ws", "RunoffWs", "Tmean8110Ws", "ElevWs", "SlopeWs", "dev_ws", "wet_ws", "agr_ws", "N_Surplus", "P_Surplus")

# All variables combined
all_vars <- c("NTL", "PTL", "DOC", "TURB", "PH", "TURB", "TEMPERATURE", "STRATIFIED", "EVAP_INFL", "lakemorpho_depth", "Precip8110Ws", "RunoffWs", "Tmean8110Ws", "ElevWs", "SlopeWs", "dev_ws", "wet_ws", "agr_ws", "N_Surplus", "P_Surplus", "N_Total_Inputs")

dep_var <- "B_G_DENS"

create_formula(dep_var, in_lake_vars, log = T)

inlake_no_re <- splm(create_formula(dep_var, in_lake_vars, log = T),
                     data = drop_na(habs, all_of(dep_var), all_of(in_lake_vars)),
                     spcov_type = "exponential", 
                     local = TRUE)

inlake_unnest_re <- splm(create_formula(dep_var, in_lake_vars, log = T),
                         data = drop_na(habs, all_of(dep_var), all_of(in_lake_vars)),
                         spcov_type = "exponential",
                         random = ~ DSGN_CYCLE + UNIQUE_ID,
                         local = TRUE)

inlake_nest_re <- splm(create_formula(dep_var, in_lake_vars, log = T),
                       data = drop_na(habs, all_of(dep_var), all_of(in_lake_vars)),
                       spcov_type = "exponential",
                       random = ~ (DSGN_CYCLE / UNIQUE_ID),
                       local = TRUE)

nolake_no_re <- splm(create_formula(dep_var, no_lake_vars, log = T),
                     data = drop_na(habs, all_of(dep_var), all_of(no_lake_vars)),
                     spcov_type = "exponential", 
                     local = TRUE)

nolake_unnest_re <- splm(create_formula(dep_var, no_lake_vars, log = T),
                         data = drop_na(habs, all_of(dep_var), all_of(no_lake_vars)),
                         spcov_type = "exponential",
                         random = ~ DSGN_CYCLE + UNIQUE_ID,
                         local = TRUE)

nolake_nest_re <- splm(create_formula(dep_var, no_lake_vars, log = T),
                       data = drop_na(habs, all_of(dep_var), all_of(no_lake_vars)),
                       spcov_type = "exponential",
                       random = ~ (DSGN_CYCLE / UNIQUE_ID),
                       local = TRUE)

# Compared models
glances(inlake_no_re, nolake_no_re, inlake_unnest_re, nolake_unnest_re, inlake_nest_re, nolake_nest_re)

# Try random forest
# A good tutorial introducing random forest https://uc-r.github.io/random_forests
modRF <- splmRF(create_formula(dep_var, all_vars, log = T),
                data = drop_na(habs, all_of(dep_var), all_of(all_vars)),
                spcov_type = "exponential",
                random = ~ (DSGN_CYCLE / UNIQUE_ID),
                local = TRUE,
                num.trees       = 500,
                mtry            = 7,
                min.node.size   = 3,
                sample.fraction = .8,
                importance = "impurity")

imp <- tibble(var = names(modRF$ranger$variable.importance), var_imp = modRF$ranger$variable.importance)

imp %>% 
  dplyr::arrange(desc(var_imp)) %>%
  # dplyr::top_n(25) %>%
  ggplot(aes(x = reorder(var, var_imp), y = var_imp)) +
  geom_col() +
  coord_flip() +
  xlab("Variable") + ylab("Variable Importance")

# Examine if the error stabilizes with the number of trees
m1 <- randomForest::randomForest(
  formula = create_formula(dep_var, all_vars, log = T),
  data = drop_na(habs, all_of(dep_var), all_of(all_vars))
)
plot(m1)

# Tune the mTry parameter
na_free <- drop_na(habs, all_of(dep_var), all_of(all_vars))

m2 <- randomForest::tuneRF(
  x          = st_drop_geometry(na_free[all_vars]),
  y          = log10(na_free$B_G_DENS + 1),
  ntreeTry   = 500,
  mtryStart  = 4,
  stepFactor = 1.5,
  improve    = 0.01,
  trace      = FALSE      # to not show real-time progress 
)

# hyperparameter grid search
hyper_grid <- expand.grid(
  mtry       = seq(4, 8, by = 1),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE   = 0
)

for(i in 1:nrow(hyper_grid)) {
  
  # train model
  model <- ranger::ranger(
    formula         = create_formula(dep_var, all_vars, log = T),
    data            = st_drop_geometry(drop_na(habs, all_of(dep_var), all_of(all_vars))), 
    num.trees       = 500,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed            = 123
  )
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

# Which set of parameters yields the lowest OOB RMSE?
hyper_grid %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(10)

# Check expected error for optimal set of hyperparameters
OOB_RMSE <- vector(mode = "numeric", length = 100)

for(i in seq_along(OOB_RMSE)) {

  optimal_ranger <- ranger::ranger(
    formula         = create_formula(dep_var, all_vars, log = T),
    data            = st_drop_geometry(drop_na(habs, all_of(dep_var), all_of(all_vars))), 
    num.trees       = 500,
    mtry            = 7,
    min.node.size   = 3,
    sample.fraction = .8,
    importance      = 'impurity'
  )
  
  OOB_RMSE[i] <- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)

imp <- tibble(var = names(optimal_ranger$variable.importance), var_imp = optimal_ranger$variable.importance)

imp %>% 
  dplyr::arrange(desc(var_imp)) %>%
  # dplyr::top_n(25) %>%
  ggplot(aes(x = reorder(var, var_imp), y = var_imp)) +
  geom_col() +
  coord_flip() +
  xlab("Variable") + ylab("Variable Importance")

# Check model performance by loocv
# These take a very long time to run. Running the nested model went for 1.25 hours and didn't finish. Should maybe explore doing a k-folds cross validation.
loocv(inlake_nest_re, local = T) # 0.8567984
loocv(nolake_nest_re, local = T)

# Split data into training and predictor sets
habs_pred <- predict(inlake_nest_re, newdata = habs, local = TRUE)

# Visualize the standardized residuals
re_nested_resid <- augment(inlake_nest_re)
mapview(re_nested_resid, zcol = ".std.resid", legend = TRUE)

# Filter to just the big residuals
tmp <- filter(re_nested_resid, .std.resid < -2 | .std.resid > 2)
mapview(tmp, zcol = ".std.resid", legend = TRUE)
# Seem fairly randomly distributed across the country

mapview(re_nested_resid, zcol = ".std.resid", at = c(-5.4, -2, 2.6), legend = TRUE)

ggplot(data = re_nested_resid, aes(x = .fitted, y = `log10(B_G_DENS + 1)`)) + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1)

ggplot(data = re_nested_resid, aes(x = .std.resid)) + 
  geom_histogram() + 

summary(mod_re_nested)
summary(mod_re_unnested)

plot(mod_re_nested)
plot(mod_re_unnested)

# Check that the spatial model performs better than a non-spatial model
inlake_nonsp <- splm(create_formula(dep_var, in_lake_vars, log = T),
                     data = drop_na(habs, all_of(dep_var), all_of(in_lake_vars)),
                     spcov_type = "none",
                     random = ~ (DSGN_CYCLE / UNIQUE_ID),
                     local = TRUE)

# Compare spatial and non-spatial models with nested random effects
glances(inlake_nonsp, inlake_nest_re)
# Hmmm, the non-spatial model performs just as well (maybe a little better) than the spatial model

# Is it because there are so many independent variables?
basic_spat <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "exponential", local = TRUE) 

basic_nonsp <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "none", local = TRUE)

glances(basic_nonsp, basic_spat)
# Reducing the number of independent variables doesn't change the answer. The spatial and non-spatial forms of the model perform about the same

# Try a few different spatial convariance types
gaussian <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "gaussian", local = TRUE) 

sphere <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "spherical", local = TRUE) 

triangular <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "triangular", local = TRUE)

circular <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "circular", local = TRUE)

cubic <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "cubic", local = TRUE)

glances(basic_nonsp, basic_spat, gaussian, sphere, triangular, circular, cubic)
# Conclusion: Exponential and cubic covariance structure perform the same and only slightly better than other form.

summary(mod_re_nested)
summary(mod_re_unnested)

tidy(mod_re_nested)
glance(mod_re_nested)
plot(mod_re_nested)
plot(mod_re_unnested)

```


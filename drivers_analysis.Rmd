---
title: "drivers_analysis"
author: "Handler"
date: '2022-11-21'
output: html_document
editor_options: 
  chunk_output_type: console
---

Based on data compiled in drivers_compilation.Rmd

```{r setup, include=FALSE}
library(usethis)
library(devtools)
library(spmodel)
library(sf)
library(mapview)
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)

# Load all data and functions from HABsDrivers package
load_all()

# Detailed guide to spmodel
# https://usepa.github.io/spmodel/articles/guide.html
```

Calculate the chlorophyll a associated with cyanobacteria

```{r}
habs <- mutate(habs, CHLA_CYANO = CHLA_RESULT * BG_BIOVOL/PHYT_BIOVOL, .after = CHLA_RESULT)
```


Examine the predictor data for colinear variables
- TN and DOC
- Precip8110Ws and Precip_Minus_EVTWs and RunoffWs
- nondev_ws and agr_ws
- SlopeWs and ElevWs
- N_Total_Inputs, N_Surplus, P_Surplus (r > 0.90)


```{r}
cormat_pred <- cor(st_drop_geometry(select(habs, TEMPERATURE:P_Surplus)), use = "pairwise.complete.obs")

# cormat_pred <- cor(st_drop_geometry(select(habs, NTL, PTL, DOC, TURB, PH, TEMPERATURE, dev_ws, wet_ws, nondev_ws, agr_ws, lakemorpho_depth, Precip8110Ws, RunoffWs, Tmean8110Ws, ElevWs, SlopeWs, N_Surplus)), use = "pairwise.complete.obs")

corrplot.mixed(cormat_pred, upper = "ellipse", diag = "n", tl.pos = "lt", tl.col = "black", tl.srt = 45)

```


How much of the variation in the response variables is potentially explainable based on the spatial versus temporal variation?


```{r}
# Assess spatial (signal) to temporal (noise) in response variables
do.call(rbind, lapply(c("MICX", "B_G_DENS", "CHLA_RESULT", "CHLA_CYANO"), function(x){
  form <- as.formula(paste(x, " ~ 1 + (1|SITE_ID)"))
  df   <- habs
  
  if(x == "MICX"){
    df <- drop_na(df, MICX)
  }
  
  if(x == "B_G_DENS"){
    form <- as.formula(paste("log10(", x, "+ 1) ~ 1 + (1|SITE_ID)"))
  }
  
  if(x == "CHLA_RESULT"){
    form <- as.formula(paste("log10(", x, ") ~ 1 + (1|SITE_ID)"))
  }
  
  if(x == "CHLA_CYANO"){
    form <- as.formula(paste("log10(", x, ") ~ 1 + (1|SITE_ID)"))
  }
  
  mod         <- lme4::lmer(form, data = df, REML = T, verbose = F)
  site.var    <- as.numeric(lme4::VarCorr(mod)) 
  visit.var   <- attr(lme4::VarCorr(mod), "sc")^2
  sn          <- site.var / visit.var
  maxR2       <- sn/(sn + 1)
  return(data.frame(variable = x, maxR2))
}))


```


Experiment with some spatial models


```{r}
# Function that creates the formula. When the response variable is cyanobacteria cell counts, add 1 and log-transform.
create_formula <- function(dep_var, ind_vars, log = F){
  # Log = F
  as.formula(paste(dep_var, " ~ ", paste(ind_vars, collapse = " + ")))
  # Log = T
  if(log == T){
    as.formula(paste("log10(", dep_var, "+ 1) ~ ", paste(ind_vars, collapse = " + ")))
  }
}

# Create variable groups
all_vars     <- colnames(habs)[match("TEMPERATURE", colnames(habs)):match("P_Surplus", colnames(habs))]
dep_vars     <- c("B_G_DENS", "MICX", "CYLSPER", "CHLA_RESULT", "CHLA_CYANO", "BG_BIOVOL", "PHYT_BIOVOL")
all_ind_vars <- all_vars[!all_vars %in% dep_vars]

# Dropping non-developed area of the watershed due to issues of colinearity with developed area. Also dropping nitrate data since it is missing for a large number of observations.
all_ind_vars <- all_ind_vars[!all_ind_vars %in% c("nondev_ws", "NITRATE_N")]

# NLA water variables
nla_ind_vars <- c("TEMPERATURE", "MAXDEPTH", "STRATIFIED", "AMMONIA_N", "DO_SURF", "DOC", "NTL", "PTL", "TURB", "NITRATE_N", "PH", "EVAP_INFL")
  
# All non-NLA variables
non_nla_vars <- all_ind_vars[!all_ind_vars %in% nla_ind_vars]

# All nutrient inventory variables
nni_vars <- colnames(habs)[match("N_AG_N2O_INPUTS", colnames(habs)):match("P_Surplus", colnames(habs))]

dep_var <- "B_G_DENS"

# 2024-01-24 Models on cyanobacteria cell counts using all available explanatory variables
# Lessons learned: 

# There is a lot of missing biovolume data. Enough that I switched back to cyano count data. Will need to check with Karen about why so much data is missing for this parameter.

# TP isn't important unless TN is removed from the model. These two variables are likely highly colinear. Watershed mean slope and elevation were coming out as highly related to cyanos, but the effect diminished with the addition of the ecoregional data as a categorical variable. It's likely that slope and elevation were acting as proxies for the western mountain and xeric ecoregions that have much lower cyanos compared to other regions. Therefore, decided to include just the three aggregated ecoregions. 

# Also, the ecoregions are included as a fixed rather than a random effect now since the effect seems to be important and not an artifact of the NLA design. 

# Might consider playing around with having an ecoregional interaction term with some of the variables. This seemed promising when there was an interaction between ecoregion and TN, but not with any other variables (tried temp and agriculture). 

# The 30-year climate normals for temp and precip always outperformed the monthly means in which the sample was collected and the water temperature at the time of sampling. This is likely at least in part to the inclusion of a survey year random effect, but when survey year is removed and monthly means included, the model performs substantially worse.

# Note that the model below explains ~55% of the variation in the cyano data while the random forest model explains ~28%.

dep_var  <- "B_G_DENS"
mod_vars <- c("NTL","DOC", "PH", "Precip8110Ws", "Tmean8110Ws", "EVAP_INFL", "MAXDEPTH", "agr_ws", "AG_ECO3")
cyano    <- splm(create_formula(dep_var, mod_vars, log = T),
                 data = drop_na(habs, all_of(dep_var), all_of(mod_vars)),
                 spcov_type = "exponential",
                 random = ~ DSGN_CYCLE + UNIQUE_ID,
                 local = T)
summary(cyano)
varcomp(cyano)

obs <- habs |>
  drop_na(all_of(dep_var), all_of(mod_vars)) |>
  st_drop_geometry() |>
  mutate(obs = log10(B_G_DENS + 1)) |>
  pull(obs)

tmp <- tibble(obs, mod = cyano$fitted$response)

ggplot(data = tmp, aes(x = obs, y = mod)) + geom_point(alpha=0.5) + xlim(0,max(tmp)) + ylim(0,max(tmp))

cyano    <- splm(log10(B_G_DENS + 1) ~ NTL + DOC + PH + Precip8110Ws + Tmean8110Ws + EVAP_INFL + MAXDEPTH + agr_ws + AG_ECO3 + NTL*AG_ECO3,
                 data = drop_na(habs, all_of(dep_var), all_of(mod_vars)),
                 spcov_type = "exponential",
                 random = ~ DSGN_CYCLE + UNIQUE_ID,
                 local = T)

#######################################################################


all_no_re_no_sp <- splm(create_formula(dep_var, all_ind_vars, log = T),
                     data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars)),
                     spcov_type = "none", 
                     local = TRUE)

all_no_re <- splm(create_formula(dep_var, all_ind_vars, log = T),
                     data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars)),
                     spcov_type = "exponential", 
                     local = TRUE)

all_re_site <- splm(create_formula(dep_var, all_ind_vars, log = T),
                         data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars)),
                         spcov_type = "exponential",
                         random = ~  UNIQUE_ID,
                         local = TRUE)

all_re_year <- splm(create_formula(dep_var, all_ind_vars, log = T),
                         data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars)),
                         spcov_type = "exponential",
                         random = ~  DSGN_CYCLE,
                         local = TRUE)

all_unnest_re <- splm(create_formula(dep_var, all_ind_vars, log = T),
                       data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars)),
                       spcov_type = "exponential",
                       random = ~ DSGN_CYCLE + UNIQUE_ID,
                       local = TRUE)

all_nest_re <- splm(create_formula(dep_var, all_ind_vars, log = T),
                       data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars)),
                       spcov_type = "exponential",
                       random = ~ (DSGN_CYCLE / UNIQUE_ID),
                       local = TRUE)

# Compare model with no spatial component, with spatial component, random effect for site alone, random effect for year along, random effect for site and year unnested, random effect for site nested within year.
# How does the AIC and R2 change with these model configurations
glances(all_no_re_no_sp,
        all_no_re,
        all_re_site,
        all_re_year,
        all_unnest_re,
        all_nest_re)

# Unnested random effects performed the best
# Model with random effect for year (no site random effect) performed almost as well with respect to AIC with slightly higher pseudo R2

nolake_no_re <- splm(create_formula(dep_var, non_nla_vars, log = T),
                     data = drop_na(habs, all_of(dep_var), all_of(non_nla_vars)),
                     spcov_type = "exponential", 
                     local = TRUE)

nolake_unnest_re <- splm(create_formula(dep_var, non_nla_vars, log = T),
                         data = drop_na(habs, all_of(dep_var), all_of(non_nla_vars)),
                         spcov_type = "exponential",
                         random = ~ DSGN_CYCLE + UNIQUE_ID,
                         local = TRUE)

nolake_nest_re <- splm(create_formula(dep_var, non_nla_vars, log = T),
                       data = drop_na(habs, all_of(dep_var), all_of(non_nla_vars)),
                       spcov_type = "exponential",
                       random = ~ (DSGN_CYCLE / UNIQUE_ID),
                       local = TRUE)


# Compare models
glances(nolake_no_re, nolake_unnest_re, nolake_nest_re)

glances(nolake_no_re, 
        nolake_unnest_re, 
        nolake_nest_re,
        all_re_year,
        all_unnest_re,
        all_nest_re)

# Explore specifying a random effect for ecoregion
# First, a model just using total N as an independent variable
ecor_test1 <- splm(create_formula(dep_var, "NTL", log = T),
                         data = drop_na(habs, all_of(dep_var), NTL),
                         spcov_type = "exponential",
                         random = ~ DSGN_CYCLE + UNIQUE_ID,
                         local = TRUE)

# Second, add in the ecoregion and allow slopes to vary by ecoregions
ecor_test2 <- splm(create_formula(dep_var, "NTL", log = T),
                         data = drop_na(habs, all_of(dep_var), NTL),
                         spcov_type = "exponential",
                         random = ~ DSGN_CYCLE + UNIQUE_ID + (NTL | AG_ECO9_NM),
                         local = TRUE)

# Add a few other potential covariates
ecor_test3 <- splm(log10(B_G_DENS + 1) ~ NTL + precip_mean_month,
                         data = drop_na(habs, B_G_DENS, NTL, precip_mean_month),
                         spcov_type = "exponential",
                         random = ~ DSGN_CYCLE + UNIQUE_ID + (NTL | AG_ECO9_NM) + (precip_mean_month | AG_ECO9_NM),
                         local = TRUE)

# How does have a slope for each ecoregion change the model?
glances(ecor_test1, ecor_test2)
varcomp(ecor_test1)
varcomp(ecor_test2)
varcomp(ecor_test3)

# Adding an ecoregion random effect where the slopes can vary
# Add a few other potential covariates

try_vars <- c("NTL", "DOC", "PH", "TEMPERATURE", "precip_mean_month", "ElevWs", "SlopeWs", "EVAP_INFL", "MAXDEPTH")

# This model has 9% independent error and 55% spatially dependent error the first time I ran it
# It had 63% independent error and <0.01% spatially dependent error the second time I ran it
# It had 18% independent error and 43% spatially dependent error the third time I ran it
ecor_test2 <- splm(create_formula(dep_var, try_vars, log = T),
                         data = drop_na(habs, all_of(dep_var), all_of(try_vars)),
                         spcov_type = "exponential",
                         random = ~ DSGN_CYCLE + UNIQUE_ID + (NTL | AG_ECO9_NM),
                         local = F)
summary(ecor_test2)
varcomp(ecor_test2)
# Need to follow up with Mike. It seems quite random how the model is partitioning independent and spatially dependent random error.

try_vars <- c("TURB", "PTL", "NTL", "DOC", "PH", "TEMPERATURE", "precip_mean_month", "ElevWs", "SlopeWs", "EVAP_INFL", "MAXDEPTH")

# This model has 9% independent error and 55% spatially dependent error the first time I ran it
# It had 63% independent error and <0.01% spatially dependent error the second time I ran it
# It had 18% independent error and 43% spatially dependent error the third time I ran it
ecor_test3 <- splm(create_formula(dep_var, try_vars, log = T),
                         data = drop_na(habs, all_of(dep_var), all_of(try_vars)),
                         spcov_type = "exponential",
                         random = ~ DSGN_CYCLE + UNIQUE_ID + AG_ECO9_NM,
                         local = F)
summary(ecor_test3)
varcomp(ecor_test3)

like_this_set <- c("NTL","DOC", "PH", "Precip8110Ws", "Tmean8110Ws", "EVAP_INFL", "MAXDEPTH", "agr_ws", "AG_ECO3")

try_vars <- c("NTL","DOC", "PH", "Precip8110Ws", "Tmean8110Ws", "EVAP_INFL", "MAXDEPTH", "agr_ws", "wet_ws", "AG_ECO3")
ecor_test4 <- splm(create_formula(dep_var, like_this_set, log = T),
                         data = drop_na(habs, all_of(dep_var), all_of(like_this_set)),
                         spcov_type = "exponential",
                         random = ~ DSGN_CYCLE + UNIQUE_ID,
                         local = T)
summary(ecor_test4)
varcomp(ecor_test4)

obs <- habs |>
  drop_na(all_of(dep_var), all_of(try_vars)) |>
  st_drop_geometry() |>
  mutate(obs = log10(B_G_DENS + 1)) |>
  pull(obs)

tmp <- tibble(obs, mod = ecor_test4$fitted$response)

ggplot(data = tmp, aes(x = obs, y = mod)) + geom_point(alpha=0.5)

# This model has 60% independent error and 4% spatially dependent error 
ecor_test3 <- splm(create_formula(dep_var, try_vars, log = T),
                         data = drop_na(habs, all_of(dep_var), all_of(try_vars)),
                         spcov_type = "exponential",
                         random = ~ DSGN_CYCLE + UNIQUE_ID + (NTL + PTL | AG_ECO9_NM),
                         local = TRUE)
summary(ecor_test3)
varcomp(ecor_test3)

# Test to see if the behavior is based on random effects
# Nope, each time I run this, the amount of variation explained by the fixed effect stays relatively steady, but the independent and spatially-dependent error terms vary wildly
test <- splm(create_formula(dep_var, try_vars, log = T),
             data = drop_na(habs, all_of(dep_var), all_of(try_vars)),
             spcov_type = "exponential",
             random = ~ DSGN_CYCLE + UNIQUE_ID + (NTL | AG_ECO9_NM),
             local = T)
summary(test)
varcomp(test)
# Got it! It the term local. When local = F (the default), I ge tthe same answer each time. When local = T, I get different answers.

# TP is never important so far. Does it become important when TN is removed from independent variables?
no_tn <- splm(create_formula(dep_var, try_vars[!try_vars == "NTL"], log = T),
             data = drop_na(habs, all_of(dep_var), all_of(try_vars)),
             spcov_type = "exponential",
             random = ~ DSGN_CYCLE + UNIQUE_ID, # + (NTL | AG_ECO9_NM),
             local = T)
summary(no_tn)
varcomp(no_tn)
# Answer is yes, when TN is removed from the model, TP becomes very important

# What if there are two nested random effects?
# When I nested a DOC random effect within ecoregion, it was no longer significant and the RE accounted for <1% of the variation. Similar effect when I do the same with watershed agriculture of BFI.
two_nre <- splm(create_formula(dep_var, try_vars, log = T),
             data = drop_na(habs, all_of(dep_var), all_of(try_vars)),
             spcov_type = "exponential",
             random = ~ DSGN_CYCLE + UNIQUE_ID + (NTL + BFIWs | AG_ECO9_NM),
             local = T)
summary(two_nre)
varcomp(two_nre)

# Starting the suspect that the survey year isn't adding much to the analysis
# When ecoregion is included as a random effect alone (no slope interactions), TP becomes significant
re2 <- splm(create_formula(dep_var, try_vars, log = T),
             data = drop_na(habs, all_of(dep_var), all_of(try_vars)),
             spcov_type = "exponential",
             random = ~ UNIQUE_ID + (NTL | AG_ECO9_NM),
             local = T)
summary(re2)
varcomp(re2)

# Check model performance by loocv
# These take a very long time to run. Running the nested model went for 1.25 hours and didn't finish. Should maybe explore doing a k-folds cross validation.
# loocv(inlake_nest_re, local = T) # 0.8567984
# loocv(nolake_nest_re, local = T)

# Split data into training and predictor sets
habs_pred <- predict(inlake_nest_re, newdata = habs, local = TRUE)

# Visualize the standardized residuals
re_nested_resid <- augment(inlake_nest_re)
mapview(re_nested_resid, zcol = ".std.resid", legend = TRUE)

# Filter to just the big residuals
tmp <- filter(re_nested_resid, .std.resid < -2 | .std.resid > 2)
mapview(tmp, zcol = ".std.resid", legend = TRUE)
# Seem fairly randomly distributed across the country

mapview(re_nested_resid, zcol = ".std.resid", at = c(-5.4, -2, 2.6), legend = TRUE)

ggplot(data = re_nested_resid, aes(x = .fitted, y = `log10(B_G_DENS + 1)`)) + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1)

ggplot(data = re_nested_resid, aes(x = .std.resid)) + 
  geom_histogram() + 

summary(mod_re_nested)
summary(mod_re_unnested)

plot(mod_re_nested)
plot(mod_re_unnested)

# Check that the spatial model performs better than a non-spatial model
inlake_nonsp <- splm(create_formula(dep_var, in_lake_vars, log = T),
                     data = drop_na(habs, all_of(dep_var), all_of(in_lake_vars)),
                     spcov_type = "none",
                     random = ~ (DSGN_CYCLE / UNIQUE_ID),
                     local = TRUE)

# Compare spatial and non-spatial models with nested random effects
glances(inlake_nonsp, inlake_nest_re)
# Hmmm, the non-spatial model performs just as well (maybe a little better) than the spatial model

# Is it because there are so many independent variables?
basic_spat <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "exponential", local = TRUE) 

basic_nonsp <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "none", local = TRUE)

glances(basic_nonsp, basic_spat)
# Reducing the number of independent variables doesn't change the answer. The spatial and non-spatial forms of the model perform about the same

# Try a few different spatial convariance types
gaussian <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "gaussian", local = TRUE) 

sphere <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "spherical", local = TRUE) 

triangular <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "triangular", local = TRUE)

circular <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "circular", local = TRUE)

cubic <- splm(log10(B_G_DENS + 1) ~ NTL, data = habs, random = ~ (DSGN_CYCLE / UNIQUE_ID), spcov_type = "cubic", local = TRUE)

glances(basic_nonsp, basic_spat, gaussian, sphere, triangular, circular, cubic)
# Conclusion: Exponential and cubic covariance structure perform the same and only slightly better than other form.

summary(mod_re_nested)
summary(mod_re_unnested)

tidy(mod_re_nested)
glance(mod_re_nested)
plot(mod_re_nested)
plot(mod_re_unnested)

```

Random forest experimenting

```{r}
# A good tutorial introducing random forest https://uc-r.github.io/random_forests

# How much missingness is there in the data?
tmp <- habs |>
  st_drop_geometry() |>
  summarize_all(funs(sum(is.na(.)))) |>
  pivot_longer(SITE_ID:UUS_L4NAME, names_to = "variable", values_to = "num_missing")


# Examine if the error stabilizes with the number of trees
m1 <- randomForest::randomForest(
  formula = create_formula(dep_var, all_ind_vars, log = T),
  data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars))
)
plot(m1)

# Tune the mTry parameter
na_free <- drop_na(habs, all_of(dep_var), all_of(all_ind_vars))

m2 <- randomForest::tuneRF(
  x          = st_drop_geometry(na_free[all_ind_vars]),
  y          = log10(na_free$B_G_DENS + 1),
  ntreeTry   = 500,
  mtryStart  = 4,
  stepFactor = 1.5,
  improve    = 0.01,
  trace      = FALSE      # to not show real-time progress 
)

# hyperparameter grid search
hyper_grid <- expand.grid(
  mtry       = seq(25, 35, by = 2),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE   = 0
)

for(i in 1:nrow(hyper_grid)) {
  
  # train model
  model <- ranger::ranger(
    formula         = create_formula(dep_var, all_ind_vars, log = T),
    data            = st_drop_geometry(drop_na(habs, all_of(dep_var), all_of(all_ind_vars))), 
    num.trees       = 500,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed            = 123
  )
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

# Which set of parameters yields the lowest OOB RMSE?
hyper_grid %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(10)

# Check expected error for optimal set of hyperparameters
OOB_RMSE <- vector(mode = "numeric", length = 100)

for(i in seq_along(OOB_RMSE)) {

  optimal_ranger <- ranger::ranger(
    formula         = create_formula(dep_var, all_ind_vars, log = T),
    data            = st_drop_geometry(drop_na(habs, all_of(dep_var), all_of(all_ind_vars))), 
    num.trees       = 500,
    mtry            = 29,
    min.node.size   = 5,
    sample.fraction = .632,
    importance      = 'impurity'
  )
  
  OOB_RMSE[i] <- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)

imp <- tibble(var = names(optimal_ranger$variable.importance), var_imp = optimal_ranger$variable.importance)

imp %>% 
  dplyr::arrange(desc(var_imp)) %>%
  ggplot(aes(x = reorder(var, var_imp), y = var_imp)) +
  geom_col() +
  coord_flip() +
  xlab("Variable") + ylab("Variable Importance") +
  ggtitle("NLA '07,'12,'17_All Variables")

ggsave("./figures/Variable importance_NLA'07,'12,'17_all.png", dpi = 300, width = 5, height = 5)


# Given the parameter tuning, now pass the data to spmodel
modRF <- splmRF(create_formula(dep_var, all_ind_vars, log = T),
                data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars)),
                spcov_type = "exponential",
                random = ~ DSGN_CYCLE + UNIQUE_ID,
                # random = ~ DSGN_CYCLE,
                local = TRUE,
                num.trees       = 500,
                mtry            = 29,
                min.node.size   = 5,
                sample.fraction = .632,
                importance = "impurity")

modRF.imp <- tibble(var = names(modRF$ranger$variable.importance), var_imp = modRF$ranger$variable.importance)

modRF.imp %>% 
  dplyr::arrange(desc(var_imp)) %>%
  dplyr::top_n(25) %>%
  ggplot(aes(x = reorder(var, var_imp), y = var_imp)) +
  geom_col() +
  coord_flip() +
  xlab("Variable") + ylab("Variable Importance")

ggsave("./figures/Variable importance_spMod_NLA'07,'12,'17_all.png", dpi = 300, width = 5, height = 5)

tmp <- habs |>
  drop_na(all_of(dep_var), all_of(all_ind_vars)) |>
  st_drop_geometry() |>
  pull(B_G_DENS)

plot(optimal_ranger$predictions, log10(tmp+1))
cor.test(optimal_ranger$predictions, log10(tmp+1))

plot(modRF$ranger$predictions, log10(tmp+1))
cor.test(modRF$ranger$predictions, log10(tmp+1))

  
```


Try random forest with large collection of data from Robert Sabo


```{r}
# All data from Robert (untransformed)
nni <- readr::read_csv("NLA07-12_NutrientInventory_Data.csv")

# Have to break out the month and day of the compiled HABs data
comp <- habs |>
  mutate(Month = as.numeric(format(as.Date(DATE_COL, format = "%m/%d/%Y"), "%m")),
         Day = as.numeric(format(as.Date(DATE_COL, format = "%m/%d/%Y"), "%d")),
         Year = as.numeric(as.character(DSGN_CYCLE))) |>
  # select(SITE_ID, Year, Month, Day, B_G_DENS, TEMPERATURE, DOC, TURB, PH, EVAP_INFL, lakemorpho_depth, ElevWs, SlopeWs) 
  # Remove redundant variables
  select(-NTL, -PTL, -RunoffWs, -LAT_DD83, -LON_DD83, -DATE_COL, -DSGN_CYCLE, -VISIT_NO, -COMID, -MICX, -CHLA_RESULT, -CYLSPER, -UNIQUE_ID)

# Merge with cyanobacteria counts
comp <- left_join(nni, comp, by = c("SITE_ID", "Year", "Month", "Day"))

# Create a vector of the independent variables
nni_vars <- colnames(comp)[c(9:91,93:(length(colnames(comp))-1))] #

# Examine if the error stabilizes with the number of trees
m1 <- randomForest::randomForest(
  formula = create_formula(dep_var, nni_vars, log = T),
  data = drop_na(comp, all_of(dep_var), all_of(nni_vars))
)
plot(m1)

# Tune the mTry parameter
na_free <- drop_na(comp, all_of(dep_var), all_of(nni_vars))

m2 <- randomForest::tuneRF(
  x          = st_drop_geometry(na_free[nni_vars]),
  y          = log10(na_free$B_G_DENS + 1),
  ntreeTry   = 500,
  mtryStart  = 4,
  stepFactor = 1.5,
  improve    = 0.01,
  trace      = FALSE      # to not show real-time progress 
)

# hyperparameter grid search
hyper_grid <- expand.grid(
  mtry       = seq(25, 35, by = 2),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE   = 0
)

for(i in 1:nrow(hyper_grid)) {
  
  # train model
  model <- ranger::ranger(
    formula         = create_formula(dep_var, nni_vars, log = T),
    data            = st_drop_geometry(drop_na(comp, all_of(dep_var), all_of(nni_vars))), 
    num.trees       = 500,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed            = 123,
    num.threads     = 8
  )
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

# Which set of parameters yields the lowest OOB RMSE?
hyper_grid %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(15)

# Check expected error for optimal set of hyperparameters
OOB_RMSE <- vector(mode = "numeric", length = 100)

for(i in seq_along(OOB_RMSE)) {

  optimal_ranger <- ranger::ranger(
    formula         = create_formula(dep_var, nni_vars, log = T),
    data            = st_drop_geometry(drop_na(comp, all_of(dep_var), all_of(nni_vars))), 
    num.trees       = 500,
    mtry            = 29,
    min.node.size   = 3,
    sample.fraction = .8,
    num.threads     = 8, 
    importance      = 'impurity'
  )
  
  OOB_RMSE[i] <- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)

optimal_ranger <- ranger::ranger(
  formula         = create_formula(dep_var, nni_vars, log = T),
  data            = drop_na(comp, all_of(dep_var), all_of(nni_vars)), 
  num.trees       = 500,
  mtry            = 29,
  min.node.size   = 3,
  sample.fraction = .8,
  num.threads     = 8, 
  importance      = 'impurity')

imp <- tibble(var = names(optimal_ranger$variable.importance), var_imp = optimal_ranger$variable.importance)

imp %>% 
  dplyr::arrange(desc(var_imp)) %>%
  dplyr::top_n(25) %>%
  ggplot(aes(x = reorder(var, var_imp), y = var_imp)) +
  geom_col() +
  coord_flip() +
  xlab("Variable") + ylab("Variable Importance") +
  ggtitle("NLA '07,'12_All Variables")

ggsave("./figures/Variable importance_NLA'07,'12_all.png", dpi = 300, width = 5, height = 5)

########################################################

# Have to break out the month and day of the compiled HABs data
comp <- habs |>
  mutate(Month = as.numeric(format(as.Date(DATE_COL, format = "%m/%d/%Y"), "%m")),
         Day = as.numeric(format(as.Date(DATE_COL, format = "%m/%d/%Y"), "%d")),
         Year = as.numeric(as.character(DSGN_CYCLE))) |>
  select(-NTL, -PTL, -RunoffWs) |>
  right_join(nni, by = c("SITE_ID", "Year", "Month", "Day")) # |>
  # filter(!duplicated(UNIQUE_ID)) # If running with unique observations

# Investigate how the year random effect interacts with some data that is specific to the sample year
nni_sub <- c(all_vars, "LSTAnomaly_YrMean", "NPP_YrMean", "Precip_YrMean", "Tmean_YrMean")

nni_no_re_no_sp <- splm(create_formula(dep_var, nni_sub, log = T),
                     data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                     spcov_type = "none", 
                     local = TRUE)

nni_no_re <- splm(create_formula(dep_var, nni_sub, log = T),
                     data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                     spcov_type = "exponential", 
                     local = TRUE)

nni_re_site <- splm(create_formula(dep_var, nni_sub, log = T),
                         data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                         spcov_type = "exponential",
                         random = ~  UNIQUE_ID,
                         local = TRUE)

nni_re_year <- splm(create_formula(dep_var, nni_sub, log = T),
                         data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                         spcov_type = "exponential",
                         random = ~  DSGN_CYCLE,
                         local = TRUE)

nni_unnest_re <- splm(create_formula(dep_var, nni_sub, log = T),
                       data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                       spcov_type = "exponential",
                       random = ~ DSGN_CYCLE + UNIQUE_ID,
                       local = TRUE)

nni_nest_re <- splm(create_formula(dep_var, nni_sub, log = T),
                       data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                       spcov_type = "exponential",
                       random = ~ (DSGN_CYCLE / UNIQUE_ID),
                       local = TRUE)

# Compare model with no spatial component, with spatial component, random effect for site alone, random effect for year along, random effect for site and year unnested, random effect for site nested within year.
# How does the AIC and R2 change with these model configurations
glances(nni_no_re_no_sp,
        nni_no_re,
        nni_re_site,
        nni_re_year,
        nni_unnest_re,
        nni_nest_re)
# Nested random effects had the lowest AIC and R2 followed by unnested random effects
# About the same amount of variation is explained with no random effects

optimal_ranger <- ranger::ranger(
  formula         = create_formula(dep_var, nni_sub, log = T),
  data            = drop_na(comp, all_of(dep_var), all_of(nni_sub)), 
  num.trees       = 500,
  mtry            = 8,
  min.node.size   = 3,
  sample.fraction = .8,
  num.threads     = 8, 
  importance      = 'impurity')

imp <- tibble(var = names(optimal_ranger$variable.importance), var_imp = optimal_ranger$variable.importance)

imp %>% 
  dplyr::arrange(desc(var_imp)) %>%
  dplyr::top_n(25) %>%
  ggplot(aes(x = reorder(var, var_imp), y = var_imp)) +
  geom_col() +
  coord_flip() +
  xlab("Variable") + ylab("Variable Importance") +
  ggtitle("NLA '07,'12_All Variables")

ggsave("./figures/Variable importance_NLA'07,'12_with annual.png", dpi = 300, width = 5, height = 5)


```



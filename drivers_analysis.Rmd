---
title: "drivers_analysis"
author: "Handler"
date: '2022-11-21'
output: html_document
editor_options: 
  chunk_output_type: console
---

Based on data compiled in drivers_compilation.Rmd

```{r setup, include=FALSE}
library(usethis)
library(devtools)
library(spmodel)
library(sf)
library(mapview)
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)

# Load all data and functions from HABsDrivers package
load_all()

# Detailed guide to spmodel
# https://usepa.github.io/spmodel/articles/guide.html
```

Calculate the chlorophyll a associated with cyanobacteria

```{r}
habs <- mutate(habs, CHLA_CYANO = CHLA_RESULT * BG_BIOVOL/PHYT_BIOVOL, .after = CHLA_RESULT)
```


Examine the predictor data for colinear variables
- TN and DOC
- Precip8110Ws and Precip_Minus_EVTWs and RunoffWs
- nondev_ws and agr_ws
- SlopeWs and ElevWs
- N_Total_Inputs, N_Surplus, P_Surplus (r > 0.90)


```{r}
cormat_pred <- cor(st_drop_geometry(select(habs, TEMPERATURE:P_Surplus)), use = "pairwise.complete.obs")

# cormat_pred <- cor(st_drop_geometry(select(habs, NTL, PTL, DOC, TURB, PH, TEMPERATURE, dev_ws, wet_ws, nondev_ws, agr_ws, lakemorpho_depth, Precip8110Ws, RunoffWs, Tmean8110Ws, ElevWs, SlopeWs, N_Surplus)), use = "pairwise.complete.obs")

corrplot.mixed(cormat_pred, upper = "ellipse", diag = "n", tl.pos = "lt", tl.col = "black", tl.srt = 45)

```


How much of the variation in the response variables is potentially explainable based on the spatial versus temporal variation?


```{r}
# Assess spatial (signal) to temporal (noise) in response variables
do.call(rbind, lapply(c("MICX", "B_G_DENS", "CHLA_RESULT", "CHLA_CYANO"), function(x){
  form <- as.formula(paste(x, " ~ 1 + (1|SITE_ID)"))
  df   <- habs
  
  if(x == "MICX"){
    df <- drop_na(df, MICX)
  }
  
  if(x == "B_G_DENS"){
    form <- as.formula(paste("log10(", x, "+ 1) ~ 1 + (1|SITE_ID)"))
  }
  
  if(x == "CHLA_RESULT"){
    form <- as.formula(paste("log10(", x, ") ~ 1 + (1|SITE_ID)"))
  }
  
  if(x == "CHLA_CYANO"){
    form <- as.formula(paste("log10(", x, ") ~ 1 + (1|SITE_ID)"))
  }
  
  mod         <- lme4::lmer(form, data = df, REML = T, verbose = F)
  site.var    <- as.numeric(lme4::VarCorr(mod)) 
  visit.var   <- attr(lme4::VarCorr(mod), "sc")^2
  sn          <- site.var / visit.var
  maxR2       <- sn/(sn + 1)
  return(data.frame(variable = x, maxR2))
}))


```

Model setup

```{r model setup}
# Function that creates the formula. When the response variable is cyanobacteria cell counts, add 1 and log-transform.
create_formula <- function(dep_var, ind_vars, log = F, interact = NULL){
  # Log = F
  form <- paste(dep_var, " ~ ", paste(ind_vars, collapse = " + "))
  # Log = T
  if(log == T){
    form <- paste("log10(", dep_var, "+ 1) ~ ", paste(ind_vars, collapse = " + "))
  }
  # Adding an interaction term
  if(is.null(interact) == F){
    form <- paste(form, "+", interact)
  }
  # Create formula object
  form_obj <- as.formula(form)
  return(form_obj)
}

# Create variable groups
all_vars     <- colnames(habs)[match("TEMPERATURE", colnames(habs)):match("P_Surplus", colnames(habs))]
dep_vars     <- c("B_G_DENS", "MICX", "CYLSPER", "CHLA_RESULT", "CHLA_CYANO", "BG_BIOVOL", "PHYT_BIOVOL")
all_ind_vars <- all_vars[!all_vars %in% dep_vars]

# Dropping non-developed area of the watershed due to issues of colinearity with developed area. Also dropping nitrate data since it is missing for a large number of observations.
all_ind_vars <- all_ind_vars[!all_ind_vars %in% c("nondev_ws", "NITRATE_N")]

# NLA water variables
nla_ind_vars <- c("TEMPERATURE", "MAXDEPTH", "STRATIFIED", "AMMONIA_N", "DO_SURF", "DOC", "NTL", "PTL", "TURB", "NITRATE_N", "PH", "EVAP_INFL")
  
# All non-NLA variables
non_nla_vars <- all_ind_vars[!all_ind_vars %in% nla_ind_vars]

# All nutrient inventory variables
nni_vars <- colnames(habs)[match("N_AG_N2O_INPUTS", colnames(habs)):match("P_Surplus", colnames(habs))]

```


Experiment with some spatial models


```{r cyano models}

# 2024-01-24 Models on cyanobacteria cell counts using all available explanatory variables
# Lessons learned: 

# There is a lot of missing biovolume data. Enough that I switched back to cyano count data. Will need to check with Karen about why so much data is missing for this parameter.

# TP isn't important unless TN is removed from the model. These two variables are likely highly colinear. Both TN and TP come out as important in the model if one of them has an interaction with three aggregated ecoregions. With TN*AG_ECO3, both the plains and western mountains come out as important plus the interaction between the plains and TN. With TP*AG_ECO3, the plains are important alone and there is an interaction between the plains and the western mountains and TP. When the interaction term is present between TN and the ecoregions, the effect of precipitation 30 yr normals and EVAP/INFL is lessened. However, the model with the lowest AIC lacks the interaction term. 

# Watershed mean slope and elevation were coming out as highly related to cyanos, but the effect diminished with the addition of the ecoregional data as a categorical variable. It's likely that slope and elevation were acting as proxies for the western mountain and xeric ecoregions that have much lower cyanos compared to other regions. Therefore, decided to include just the three aggregated ecoregions. 

# Also, the ecoregions are included as a fixed rather than a random effect now since the effect seems to be important and not an artifact of the NLA design. 

# Might consider playing around with having an ecoregional interaction term with some of the variables. This seemed promising when there was an interaction between ecoregion and TN, but not with any other variables (tried temp and agriculture). 

# The 30-year climate normals for temp and precip always outperformed the monthly means in which the sample was collected and the water temperature at the time of sampling. This is likely at least in part to the inclusion of a survey year random effect, but when survey year is removed and monthly means included, the model performs substantially worse.

# Note that in some preliminary tests the spatial regression models are explianing ~40-55% of the variation cyano count data while the random forest model explains ~28%.

dep_var  <- "B_G_DENS"
mod_vars <- c("NTL","DOC", "PH", "Precip8110Ws", "Tmean8110Ws", "EVAP_INFL", "MAXDEPTH", "agr_ws", "AG_ECO3")
cyano    <- splm(create_formula(dep_var, mod_vars, log = T),
                 data = drop_na(habs, all_of(dep_var), all_of(mod_vars)),
                 spcov_type = "exponential",
                 random = ~ DSGN_CYCLE + UNIQUE_ID,
                 local = T)

cyano_c <- splm(create_formula(dep_var, mod_vars, log = T, interact = "AG_ECO3*PTL"),
                 data = drop_na(habs, all_of(dep_var), all_of(mod_vars)),
                 spcov_type = "exponential",
                 random = ~ DSGN_CYCLE + UNIQUE_ID,
                 local = T)

summary(cyano_c)
varcomp(cyano_c)

# cyano1 <- cyano

cyano2    <- splm(create_formula(dep_var, mod_vars, log = T),
                 data = drop_na(habs, all_of(dep_var), all_of(mod_vars)),
                 spcov_type = "exponential",
                 random = ~ DSGN_CYCLE + UNIQUE_ID,
                 local = F)
summary(cyano2)
varcomp(cyano2)

obs <- habs |>
  drop_na(all_of(dep_var), all_of(mod_vars)) |>
  st_drop_geometry() |>
  mutate(obs = log10(B_G_DENS + 1)) |>
  pull(obs)

tmp <- tibble(obs, mod = cyano2$fitted$response)

ggplot(data = tmp, aes(x = obs, y = mod)) + geom_point(alpha=0.5) + xlim(0,max(tmp,obs)) + ylim(0,max(tmp, obs))

cor.test(tmp$obs, tmp$mod)

```


Models for microcystin detection (presence/absence)


```{r}
# Create the binary microcystin variable
habs <- mutate(habs, MICX_DET = factor(ifelse(is.finite(MICX), 1, 0)))

dep_var  <- "MICX_DET"

run_micx <- function(mod_vars, int = NULL){
  spglm(create_formula(dep_var, mod_vars, log = F, interact = int),
                family = "binomial",
                data = drop_na(habs, all_of(dep_var), all_of(mod_vars)),
                spcov_type = "exponential",
                random = ~ DSGN_CYCLE + UNIQUE_ID,
                local = T)
}

# Lessons learned
# These models take substantially longer to run than the linear models and the estimated covariates are most unstable when run using approximation methods (local = T).



# These were all run with local = T
micx1 <- run_micx(mod_vars = c("NTL","PTL", "DOC", "PH", "dev_ws", "agr_ws", "KffactWs", "precip_mean_month", "TEMPERATURE", "BFIWs", "MAXDEPTH", "EVAP_INFL", "lakemorpho_fetch", "AG_ECO3"))

micx2 <- run_micx(mod_vars = c("NTL","PTL", "DOC", "PH", "dev_ws", "agr_ws", "wet_ws", "KffactWs", "precip_mean_month", "temp_mean_month", "BFIWs", "MAXDEPTH", "EVAP_INFL", "lakemorpho_fetch", "AG_ECO3"), int = "PTL*AG_ECO3")

micx3 <- run_micx(mod_vars = c("NTL","PTL", "DOC", "PH", "dev_ws", "agr_ws", "KffactWs", "precip_mean_month", "temp_mean_month", "BFIWs", "MAXDEPTH", "EVAP_INFL", "lakemorpho_fetch", "AG_ECO3"), int = "NTL*AG_ECO3")

micx4 <- run_micx(mod_vars = c("NTL", "DOC", "PH", "dev_ws", "agr_ws", "KffactWs", "precip_mean_month", "temp_mean_month", "BFIWs", "MAXDEPTH", "EVAP_INFL", "lakemorpho_fetch", "AG_ECO3"))

micx5 <- run_micx(mod_vars = c("NTL", "PH", "dev_ws", "agr_ws", "KffactWs", "temp_mean_month", "BFIWs", "MAXDEPTH", "EVAP_INFL", "lakemorpho_fetch", "AG_ECO3"))

# Set local = F here since it seems like the estimates are changing quite a bit between estimated runs
micx6 <- run_micx(mod_vars = c("NTL", "PH", "dev_ws", "agr_ws", "KffactWs", "temp_mean_month", "BFIWs", "MAXDEPTH", "EVAP_INFL", "lakemorpho_fetch", "AG_ECO3"))

# Back to local = T
micx7 <- run_micx(mod_vars = c("NTL", "PH", "dev_ws", "agr_ws", "KffactWs", "temp_mean_month", "BFIWs", "MAXDEPTH", "EVAP_INFL", "lakemorpho_fetch", "RunoffWs", "AG_ECO3"))

summary(micx6)
varcomp(micx7)

aug_micx <- augment(micx6)

pROC::auc(aug_micx$MICX_DET, aug_micx$.fitted)


```


Models for microcystin concentration, when present


```{r}
dep_var  <- "MICX"

run_micxlm <- function(mod_vars, int = NULL){
  splm(create_formula(dep_var, mod_vars, log = T, interact = int),
                data = drop_na(habs, all_of(dep_var), all_of(mod_vars)),
                spcov_type = "exponential",
                random = ~ DSGN_CYCLE + UNIQUE_ID,
                local = T)
}

# Lessons learned
# Similarly to cyanobacteria cell counts, TP only becomes important when TN is removed as a covariate, signaling that these are colinear.

micxlm1 <- run_micxlm(mod_vars = c("NTL","PTL", "DOC", "PH", "wet_ws", "agr_ws", "precip_mean_month", "TEMPERATURE", "BFIWs", "RunoffWs", "EVAP_INFL", "MAXDEPTH", "lakemorpho_shoreline.length", "AG_ECO3"))

micxlm2 <- run_micxlm(mod_vars = c("NTL","PTL", "DOC", "PH", "wet_ws", "agr_ws", "precip_mean_month", "TEMPERATURE", "BFIWs", "RunoffWs", "EVAP_INFL", "MAXDEPTH", "lakemorpho_shoreline.length", "AG_ECO3"), int = "NTL*AG_ECO3")

micxlm3 <- run_micxlm(mod_vars = c("NTL","PTL", "DOC", "PH", "wet_ws", "agr_ws", "precip_mean_month", "TEMPERATURE", "BFIWs", "RunoffWs", "EVAP_INFL", "MAXDEPTH", "lakemorpho_shoreline.length", "AG_ECO3"), int = "PTL*AG_ECO3")

micxlm4 <- run_micxlm(mod_vars = c("NTL", "DOC", "PH", "wet_ws", "agr_ws", "precip_mean_month", "TEMPERATURE", "BFIWs", "RunoffWs", "EVAP_INFL", "MAXDEPTH", "lakemorpho_shoreline.length", "AG_ECO3"))

micxlm5 <- run_micxlm(mod_vars = c("PTL", "DOC", "PH", "wet_ws", "agr_ws", "precip_mean_month", "TEMPERATURE", "BFIWs", "RunoffWs", "EVAP_INFL", "MAXDEPTH", "lakemorpho_shoreline.length", "AG_ECO3"))

micxlm6 <- run_micxlm(mod_vars = c("NTL", "DOC", "PH", "agr_ws", "precip_mean_month", "TEMPERATURE", "BFIWs", "RunoffWs", "EVAP_INFL", "MAXDEPTH", "lakemorpho_shoreline.length", "AG_ECO3"))

micxlm7 <- run_micxlm(mod_vars = c("NTL", "DOC", "PH", "agr_ws", "precip_mean_month", "TEMPERATURE", "BFIWs", "EVAP_INFL", "MAXDEPTH", "lakemorpho_shoreline.length"))

micxlm8 <- run_micxlm(mod_vars = c("NTL", "DOC", "PH", "agr_ws", "precip_mean_month", "TEMPERATURE", "BFIWs", "EVAP_INFL", "MAXDEPTH"))

micxlm9 <- run_micxlm(mod_vars = c("NTL", "DOC", "PH", "agr_ws", "precip_mean_month", "Tmean8110Ws", "BFIWs", "EVAP_INFL", "MAXDEPTH"))

micxlm10 <- run_micxlm(mod_vars = c("NTL", "DOC", "PH", "agr_ws", "precip_mean_month", "Tmean8110Ws", "BFIWs", "EVAP_INFL", "lakemorpho_fetch"))

micxlm11 <- run_micxlm(mod_vars = c("NTL", "DOC", "PH", "agr_ws", "precip_mean_month", "Tmean8110Ws", "EVAP_INFL"))

micxlm12 <- run_micxlm(mod_vars = c("NTL", "DOC", "PH", "TURB", "agr_ws", "precip_mean_month", "Tmean8110Ws", "EVAP_INFL"))

micxlm13 <- run_micxlm(mod_vars = c("NTL", "DOC", "PH", "TURB", "agr_ws", "precip_mean_month", "Tmean8110Ws", "EVAP_INFL", "ElevWs"))

summary(micxlm13)
varcomp(micxlm12)

obs <- habs |>
  drop_na(all_of(dep_var), all_of(mod_vars)) |>
  st_drop_geometry() |>
  mutate(obs = log10(MICX + 1)) |>
  pull(obs)

tmp <- tibble(obs, mod = micxlm12$fitted$response)

ggplot(data = tmp, aes(x = obs, y = mod)) + geom_point(alpha=0.5) + xlim(0,max(tmp,obs)) + ylim(0,max(tmp, obs))

cor.test(tmp$obs, tmp$mod)

```



Random forest experimenting

```{r}
# A good tutorial introducing random forest https://uc-r.github.io/random_forests

# Create a binary for microcystin detections
# habs <- mutate(habs, MICX_DET = factor(ifelse(is.finite(MICX), 1, 0)))

# Dependent variable 
dep_var <- "MICX"

# How much missingness is there in the data?
missing_data <- habs |>
  st_drop_geometry() |>
  summarize_all(funs(sum(is.na(.)))) |>
  pivot_longer(SITE_ID:UUS_L4NAME, names_to = "variable", values_to = "num_missing")

# Examine if the error stabilizes with the number of trees
m1 <- randomForest::randomForest(
  formula = create_formula(dep_var, all_ind_vars, log = F),
  data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars))
)
plot(m1)

# Tune the mTry parameter
na_free <- drop_na(habs, all_of(dep_var), all_of(all_ind_vars))

m2 <- randomForest::tuneRF(
  x          = st_drop_geometry(na_free[all_ind_vars]),
  y          = na_free$MICX,
  ntreeTry   = 500,
  mtryStart  = 4,
  stepFactor = 1.5,
  improve    = 0.01,
  trace      = FALSE      # to not show real-time progress 
)

# hyperparameter grid search
hyper_grid <- expand.grid(
  mtry       = seq(4, 16, by = 2),
  node_size  = seq(2, 8, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE   = 0
)

for(i in 1:nrow(hyper_grid)) {
  
  # train model
  model <- ranger::ranger(
    formula         = create_formula(dep_var, all_ind_vars, log = F),
    data            = st_drop_geometry(drop_na(habs, all_of(dep_var), all_of(all_ind_vars))), 
    num.trees       = 500,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed            = 123
  )
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

# Which set of parameters yields the lowest OOB RMSE?
hyper_grid %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(10)

# Check expected error for optimal set of hyperparameters
OOB_RMSE <- vector(mode = "numeric", length = 100)

for(i in seq_along(OOB_RMSE)) {

  optimal_ranger <- ranger::ranger(
    formula         = create_formula(dep_var, all_ind_vars, log = F),
    data            = st_drop_geometry(drop_na(habs, all_of(dep_var), all_of(all_ind_vars))), 
    num.trees       = 500,
    mtry            = 8,
    min.node.size   = 4,
    sample.fraction = .8,
    importance      = 'impurity'
  )
  
  OOB_RMSE[i] <- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)

imp <- tibble(var = names(optimal_ranger$variable.importance), var_imp = optimal_ranger$variable.importance)

imp %>% 
  dplyr::arrange(desc(var_imp)) %>%
  ggplot(aes(x = reorder(var, var_imp), y = var_imp)) +
  geom_col() +
  coord_flip() +
  xlab("Variable") + ylab("Variable Importance") +
  ggtitle("NLA '07,'12,'17_All Variables")

# ggsave("./figures/Variable importance_NLA'07,'12,'17_all.png", dpi = 300, width = 5, height = 5)


# Given the parameter tuning, now pass the data to spmodel
modRF <- splmRF(create_formula(dep_var, all_ind_vars, log = T),
                data = drop_na(habs, all_of(dep_var), all_of(all_ind_vars)),
                spcov_type = "exponential",
                random = ~ DSGN_CYCLE + UNIQUE_ID,
                local = TRUE,
                num.trees       = 500,
                mtry            = 8,
                min.node.size   = 4,
                sample.fraction = .8,
                importance = "impurity")

modRF.imp <- tibble(var = names(modRF$ranger$variable.importance), var_imp = modRF$ranger$variable.importance)

modRF.imp %>% 
  dplyr::arrange(desc(var_imp)) %>%
  dplyr::top_n(25) %>%
  ggplot(aes(x = reorder(var, var_imp), y = var_imp)) +
  geom_col() +
  coord_flip() +
  xlab("Variable") + ylab("Variable Importance")

# ggsave("./figures/Variable importance_spMod_NLA'07,'12,'17_all.png", dpi = 300, width = 5, height = 5)

obs <- habs |>
  drop_na(all_of(dep_var), all_of(all_ind_vars)) |>
  st_drop_geometry() |>
  mutate(obs = log10(B_G_DENS +1)) |>
  pull(obs)

plot(optimal_ranger$predictions, obs)
cor.test(optimal_ranger$predictions, log10(tmp+1))

mod <- modRF$ranger$predictions

plot(obs, mod, ylim = c(0, max(mod, obs)), xlim = c(0, max(mod, obs)))
cor.test(mod, obs)

  
```


Try random forest with large collection of data from Robert Sabo


```{r}
# All data from Robert (untransformed)
nni <- readr::read_csv("NLA07-12_NutrientInventory_Data.csv")

# Have to break out the month and day of the compiled HABs data
comp <- habs |>
  mutate(Month = as.numeric(format(as.Date(DATE_COL, format = "%m/%d/%Y"), "%m")),
         Day = as.numeric(format(as.Date(DATE_COL, format = "%m/%d/%Y"), "%d")),
         Year = as.numeric(as.character(DSGN_CYCLE))) |>
  # select(SITE_ID, Year, Month, Day, B_G_DENS, TEMPERATURE, DOC, TURB, PH, EVAP_INFL, lakemorpho_depth, ElevWs, SlopeWs) 
  # Remove redundant variables
  select(-NTL, -PTL, -RunoffWs, -LAT_DD83, -LON_DD83, -DATE_COL, -DSGN_CYCLE, -VISIT_NO, -COMID, -MICX, -CHLA_RESULT, -CYLSPER, -UNIQUE_ID)

# Merge with cyanobacteria counts
comp <- left_join(nni, comp, by = c("SITE_ID", "Year", "Month", "Day"))

# Create a vector of the independent variables
nni_vars <- colnames(comp)[c(9:91,93:(length(colnames(comp))-1))] #

# Examine if the error stabilizes with the number of trees
m1 <- randomForest::randomForest(
  formula = create_formula(dep_var, nni_vars, log = T),
  data = drop_na(comp, all_of(dep_var), all_of(nni_vars))
)
plot(m1)

# Tune the mTry parameter
na_free <- drop_na(comp, all_of(dep_var), all_of(nni_vars))

m2 <- randomForest::tuneRF(
  x          = st_drop_geometry(na_free[nni_vars]),
  y          = log10(na_free$B_G_DENS + 1),
  ntreeTry   = 500,
  mtryStart  = 4,
  stepFactor = 1.5,
  improve    = 0.01,
  trace      = FALSE      # to not show real-time progress 
)

# hyperparameter grid search
hyper_grid <- expand.grid(
  mtry       = seq(25, 35, by = 2),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE   = 0
)

for(i in 1:nrow(hyper_grid)) {
  
  # train model
  model <- ranger::ranger(
    formula         = create_formula(dep_var, nni_vars, log = T),
    data            = st_drop_geometry(drop_na(comp, all_of(dep_var), all_of(nni_vars))), 
    num.trees       = 500,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed            = 123,
    num.threads     = 8
  )
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

# Which set of parameters yields the lowest OOB RMSE?
hyper_grid %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(15)

# Check expected error for optimal set of hyperparameters
OOB_RMSE <- vector(mode = "numeric", length = 100)

for(i in seq_along(OOB_RMSE)) {

  optimal_ranger <- ranger::ranger(
    formula         = create_formula(dep_var, nni_vars, log = T),
    data            = st_drop_geometry(drop_na(comp, all_of(dep_var), all_of(nni_vars))), 
    num.trees       = 500,
    mtry            = 29,
    min.node.size   = 3,
    sample.fraction = .8,
    num.threads     = 8, 
    importance      = 'impurity'
  )
  
  OOB_RMSE[i] <- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)

optimal_ranger <- ranger::ranger(
  formula         = create_formula(dep_var, nni_vars, log = T),
  data            = drop_na(comp, all_of(dep_var), all_of(nni_vars)), 
  num.trees       = 500,
  mtry            = 29,
  min.node.size   = 3,
  sample.fraction = .8,
  num.threads     = 8, 
  importance      = 'impurity')

imp <- tibble(var = names(optimal_ranger$variable.importance), var_imp = optimal_ranger$variable.importance)

imp %>% 
  dplyr::arrange(desc(var_imp)) %>%
  dplyr::top_n(25) %>%
  ggplot(aes(x = reorder(var, var_imp), y = var_imp)) +
  geom_col() +
  coord_flip() +
  xlab("Variable") + ylab("Variable Importance") +
  ggtitle("NLA '07,'12_All Variables")

ggsave("./figures/Variable importance_NLA'07,'12_all.png", dpi = 300, width = 5, height = 5)

########################################################

# Have to break out the month and day of the compiled HABs data
comp <- habs |>
  mutate(Month = as.numeric(format(as.Date(DATE_COL, format = "%m/%d/%Y"), "%m")),
         Day = as.numeric(format(as.Date(DATE_COL, format = "%m/%d/%Y"), "%d")),
         Year = as.numeric(as.character(DSGN_CYCLE))) |>
  select(-NTL, -PTL, -RunoffWs) |>
  right_join(nni, by = c("SITE_ID", "Year", "Month", "Day")) # |>
  # filter(!duplicated(UNIQUE_ID)) # If running with unique observations

# Investigate how the year random effect interacts with some data that is specific to the sample year
nni_sub <- c(all_vars, "LSTAnomaly_YrMean", "NPP_YrMean", "Precip_YrMean", "Tmean_YrMean")

nni_no_re_no_sp <- splm(create_formula(dep_var, nni_sub, log = T),
                     data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                     spcov_type = "none", 
                     local = TRUE)

nni_no_re <- splm(create_formula(dep_var, nni_sub, log = T),
                     data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                     spcov_type = "exponential", 
                     local = TRUE)

nni_re_site <- splm(create_formula(dep_var, nni_sub, log = T),
                         data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                         spcov_type = "exponential",
                         random = ~  UNIQUE_ID,
                         local = TRUE)

nni_re_year <- splm(create_formula(dep_var, nni_sub, log = T),
                         data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                         spcov_type = "exponential",
                         random = ~  DSGN_CYCLE,
                         local = TRUE)

nni_unnest_re <- splm(create_formula(dep_var, nni_sub, log = T),
                       data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                       spcov_type = "exponential",
                       random = ~ DSGN_CYCLE + UNIQUE_ID,
                       local = TRUE)

nni_nest_re <- splm(create_formula(dep_var, nni_sub, log = T),
                       data = drop_na(comp, all_of(dep_var), all_of(nni_sub)),
                       spcov_type = "exponential",
                       random = ~ (DSGN_CYCLE / UNIQUE_ID),
                       local = TRUE)

# Compare model with no spatial component, with spatial component, random effect for site alone, random effect for year along, random effect for site and year unnested, random effect for site nested within year.
# How does the AIC and R2 change with these model configurations
glances(nni_no_re_no_sp,
        nni_no_re,
        nni_re_site,
        nni_re_year,
        nni_unnest_re,
        nni_nest_re)
# Nested random effects had the lowest AIC and R2 followed by unnested random effects
# About the same amount of variation is explained with no random effects

optimal_ranger <- ranger::ranger(
  formula         = create_formula(dep_var, nni_sub, log = T),
  data            = drop_na(comp, all_of(dep_var), all_of(nni_sub)), 
  num.trees       = 500,
  mtry            = 8,
  min.node.size   = 3,
  sample.fraction = .8,
  num.threads     = 8, 
  importance      = 'impurity')

imp <- tibble(var = names(optimal_ranger$variable.importance), var_imp = optimal_ranger$variable.importance)

imp %>% 
  dplyr::arrange(desc(var_imp)) %>%
  dplyr::top_n(25) %>%
  ggplot(aes(x = reorder(var, var_imp), y = var_imp)) +
  geom_col() +
  coord_flip() +
  xlab("Variable") + ylab("Variable Importance") +
  ggtitle("NLA '07,'12_All Variables")

ggsave("./figures/Variable importance_NLA'07,'12_with annual.png", dpi = 300, width = 5, height = 5)


```


